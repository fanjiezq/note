# 简介
从狭义上说，hadoop是一个高效可靠的分布式存储计算，其包含三个组件 hdfs，mapreduce，yarn，分别进行分布式存储，分布式计算，分布式资源调度。
从广义上说，hadoop是一个生态圈，泛指大数据相关的框架技术，包含hadoop，hive，hbase，zookeeper，kafka等等

# hadoop1.x vs hadoop1.x vs hadoop3.x
## 架构差别
+ hadoop1 由两个组件组成，hdfs 和 mapreduce。hdfs 负责分布式存储，主从架构，有且仅有一个namenode，多个datanode，因为namenode是单点的存在单点故障，单点内存有限，集群规模收到限制，无法很好的扩展；mapreduce负责分布式计算，也是主从架构，一个JobTracker和多个TaskTracker，JobTracker负责任务的分发和监控。一个集群只有一个JobTracker，也会出现单点故障，而且因为JobTracker做了太多事情很容易成为风险点和性能瓶颈
+ hadoop2 由 hdfs，mapreduce，yarn。mapreduce只负责计算，yarn负责资源管理和调度，yarn的存在使得资源管理被独立出来，集群的各个角色功能划分更清晰明确，且yarn可以高可用部署，解决了单点故障，mapreduce也使用ApplicationMaster进行作业的启动和监控，也解决了单点故障。yarn本身作为分布式资源管理平台还可以运行其他分布式计算框架
+ hadoop3 在架构上并无变化，主要在于系统优化。1）使用擦除编码方式减少存储开销 2） namenode 支持多个备份节点 3）Yarn资源类型可定义了，不仅仅是cpu和内存 

# 概念介绍
## HDFS
+ NameNode:集群文件的管理者，保存了文件的元数据和文件与数据位置的映射关系，负责处理客户端请求，数据读写都要请求namenode。可以部署为高可用，但同一时间只有一个namenode对外服务，还可以部署为邦联模式，扩大集群规模。高可用模式下的从节点可以完成checkpount操作，因此没必要配置Secondary NameNode
+ DataNode:实际存储数据的节点，每个节点都有一个DataNode进程管理着本节点的数据，数据以block形式管理，默认1288M，客户端定位到文件位置后直接从DataNode读写数据。
+ ZKFC:DFSZKFailoverController 高可用模式下 namenode 的监控服务，以心跳的方式监控namenode,如果发现主节点失效，会重新选举一个主节点。ZKFC 本身也是集群部署的，不会有单点鼓掌 
+ JournalNode:集群模式下用于namenode 主从节点的数据同步，作用相当于一个共享文件系统，主节点每遇到文件修改都会往里写editlog数据，从节点从里面同步数据。JournalNode 是集群部署的，只要主节点在 JournalNode 写入成功就不必担心数据丢失了

## YARN
+ ResourceManager:集群的资源管理者，负责接收客户端的资源申请，进行资源的分配，回收和监控，资源一般为cpu和内存
    - Scheduler:资源调度器，专门负责ResourceManager的资源分配工作，以container列表的形式调度资源
    - ApplicationsManager:负责接收client的作业提交，并且协商如何启动ApplicationMaster，以及Job的监控，和重启
+ NodeManager:集群的每个节点都会运行一个 NodeManager 进程，负责定时汇报节点状态和资源使用情况，同时接收客户端的计算请求，运行计算任务，汇报任务完成进度
    - YarnChild:真正的任务执行者，NodeManager接收到来自AM的执行任务请求就会执行 YarnChild 的main方法运行Task
+ ApplicationMaster:每个application都需要一个ApplicationMaster用于和namenode申请资源，与datanode协商任务执行
+ Container:资源的抽象，ResourceManager进行资源分配的基本单位，一般是封装为 cpu + 内存的形式，Task执行是在container内执行的，一个Task不可以使用超过container描述的资源，否则会被强制杀死

# HDFS文件块为什么这么大
Hadoop1.X的文件块64M,hadoop2.X文件块默认128M,之所以这么大是为了减少寻址开销，因为HDFS定位文件时间比普通本地文件系统要长不少，如果文件块太小，文件定位时间在整个文件读取过程中占比太多，性能受到影响。但是这个块也不应该设置的太大，因为计算时一个数据块对应一个map任务，数据块太大会导致map任务太少，不能发挥集群并行计算的优势

# HFDS文件读取流程
1. 客户端通过Distributed FileSystem模块请求Namenode获取文件位置
2. namenode根据文件名查询到文件block的位置，并按照与客户端的位置排序，然后将block位置列表返回给客户端，
3. 客户端首先读取距离最近的起始块，以packet为单位传输(默认64K)，完成后关闭对应datanode连接读取下一个数据块
4. 每次读取数据块客户端都尽量从距离自己最近的节点读取，但是这些选择不步骤对用户来说是透明的，在用户看来，我们一直在读取一个连续的流。如果遇到读取错误，客户端会尝试从另外一个临近的节点读取。在读取时客户端也会检查文件的完整性，如果数据块有损坏，客户端会报告给namenode

# HDFS 文件写入流程
1. 客户端通过Distributed FileSystem模块向namenode请求创建一个文件，namenode检查文件是否已经存在，如果通过检查，直接先将创建文件操作写入EditLog，并返回输出流对象FSDataOutputStream
2. 客户端通过Distributed FileSystem模块再请求namenode请求文件的存储节点位置，一般返回三个datanode节点(dn1.dn2,dn3) 
3. FSDataOutputStream将文件拆分为packet，保存在一个数据队列，开始发送到dn1,dn1 收到一个packet就会传给dn2，dn2就会传给dn3,形成一个pipeline。每传递一个packet，datanode都会返回一个确认消息表示数据写入成功，确认消息是按照pipeline路径逆向报告(dn3 -> dn2 -> dn1 -> client)，针对每个packet，客户端只会收到一个确认消息
4. 当第一个block传递完成，客户端会请求第二个block的存储位置，并重复上述步骤。每个DataNode写完数据都会返回确认信息，客户端根据配置的数据一致性要求(强一致性还是最终一致性)判断数据是否写入成功。
    - 强一致性：所有datanode节点都返回确认信息，客户端才认为这个block写入成功
    - 最终一致性：只要有一个datanode节点返回确认信息，客户端就认为这个block写入成功
5. 客户端收到pipeline中所有datanode节点的确认后通知namenode

# HDFS写文件故障恢复
hdfs在写文件是可能存在三种故障，1）namenode故障 2) 客户端故障 3）datanode故障 ,三种故障分别有其处理方式
+ namenode故障: 如果在写写入过程中namenode故障，由于datanode已经被分配了，block的写入过程并不会受影响，但是最后一部的确认通知会失败，整个写入是无效的，如果是单点模式需要人工干预
+ 客户端故障:Lease Recovery:客户端在写文件时需要首先向namenode申请lease(租约)，类似一把锁，只有获取到lease才可以进行写操作。且为了保证客户端鼓掌时lease长期被占用，此租约有时间限制，到期必须续租，否则其他客户端可能覆盖这个文件，也可能namenode尝试恢复文件，如果文件无法恢复，租约到期，这个文件可能被任何客户端写入覆盖
+ datanode故障:Pipeline Recovery: 在数据写入过程中datanode可能异常，一个datanode异常就会导致pipeline失效，集群会尝试如下措施恢复错误 
    1. 客户端断开pipeline，并且将确认队列的数据全部加入到数据队列，保证所有packet都要重新确认
    2. 完好的两个datanode的block数据id升级，保证在坏datanode恢复后，namenode会将它不完整的block清除
    3. client重新建立pipeline,如果配置了datanode替换策略，则会namenode重新申请一个datanode,并和剩下的datanode组成一个新的pipeline，客户端会从之前返回的位置继续传递数据，但是之前确认队列全部需要再次确认

# 作业提交流程
1. 客户端向resourcemanager申请一个AppId
2. 获取到AppId后客户端对数据进行切片，切片完成后将资源和jar包发送到集群，文件夹以AppId命名
3. 封装job，向resourcemanager提交job
4. ApplicationsManager 收到job请求后，让Scheduler调度器分配一个Container，启动ApplicationMaster
5. ApplicationMaster 启动完成后开始以心跳的方式向RM请求资源运行Task。如果作业很小(默认10个map任务，1个reduce人物，处理文件小于一个block) ApplicationMaster会在本地直接执行计算
6. ApplicationMaster获取到Contioner列表，向对应的NodeManager发送任务执行请求
7. NodeManager 接收到请求，执行 YarnChild 的main方法，运行Task。在任务执行期间。Task会每3秒钟向AM汇报一次任务进度
8. map任务和reduce任务都计算完成，AM把任务状态改为成功，归还Container

# 失败处理
+ 任务运行失败:Task可能因为异常退出运行，此时其JVM会向AM报告异常状态；Task也可能因为JVM出错直接退出，此时DataNode会注意到此异常，想AM报告该任务异常状态。AM得知一个任务失败后会进行重试，而且避免在出错的节点运行，如果重试4次(可配置)依旧失败，整个任务都会失败(可以配置为部分成功)
+ ApplicationMaster异常:AM启动后会以心跳的形式告诉ApplicationsManager自己的状态，如果心跳超时，ApplicationsManager会判定AM失败，并申请一个新的容器启动新的AM，默认重启两次，超过则不再重试，整个作业失败。在任务运行过程中客户端会轮询进度报告，正常情况客户端向ResourceManager查询到AM的位置后缓存起来，但是AM失败重启后位置失效了，客户端会重新向ApplicationsManager请求AM的位置
+ NodeManager失败:集群启动后DM会以心跳的形式向RM报告自己的状态，如果超时没有汇报(默认10分钟)，RM判定DM失败，RM会从自己管理的节点池中移除这一节点。节点上运行的任务全部失败，按照以上两个场景进行恢复。如果失败次数过多(默认3次)，该节点会被RM拉黑
+ 单机模式的RM失败只能手动干预，部署为高可用模式，RM以主从模式存在，主节点失效后从节点立即接替。RM关于应用程序的信息都存储在ZK或者HDFS，所以RM的切换不会造成任务数据的丢失
