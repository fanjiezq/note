# Yarn
# namenode 数据持久化
+ namenode是HDFS的核心，早期namenode是单点的，存在单点故障，所以数据持久化的就是非常必要，否则一旦namenode出问题，内存中的数据全部丢失，整个集群就无法使用且无法恢复
+ namenode数据的持久化依赖两种文件。一种是快照文件(fsimage),一种是写入日志(edit log).在namenode运行过程中，所有的写入操作都会记录在edit log，但是不会同步到fsimage，在namenode重启的时候两者进行合并，形成一个最新的fsimage
+ fsimage + edit log 的持久化方案存在一个很大的缺陷，就是hdfs集群很少重启，这会导致edit log迟迟不与fsimage合并，edit log会变得非常大，一旦系统重启，重启过程会非常长
+ Secondary NameNode 解决了 edit log 长时间不与 fsimage 文件合并的问题。Secondary NameNode 是运行在Namenode节点之外的独立进程，它会定时通知Namenode进行文件合并，并且拉去namenode的两个持久化文件，在本地合并好了以后再发回Namenode替换掉旧的fsimage。

## 资源调度策略
集群的资源是有限的，面对大量的应用请求，资源的调度就成为一大问题，所有人都觉得自己的应用应该优先处理，所以没有最好的调度策略，必须结合具体的场景制定调度策略。Yarn提供了三种调度策略
+ FIFO调度策略:资源调度遵循先来先执行原则，其优点是实现简单，系统吞吐量大，缺点是大作业会持续占用资源，导致小作业产生饥饿
+ 容量调度策略:将集群资源按照队列划分为多个部分，每个队列又可以继续划分层次，job在提交时可以指定自己使用的队列，执行时只能使用本队列的资源
+ 公平调度策略:公平调度策略采用一种动态调度机制，A启动一个作业会先占用集群所有资源，此时B启动一个作业，会占用集群1/2资源，B再启动第二个作业，那么B的两个作业会平分1/2资源
+ 以上三种调度策略可以相互结合，比如先在最顶层使用容量调度策略将集群分为不同的几个队列，每个队列中再使用FIFO调度策略或者公平调度策略
    - 队列放置:如果集群顶层采用公平调度策略将集群分为多个队列，那么多个队列之间的分配是公平的，意味着应用会被公平的调度到这几个队列，那么具体调度到哪个队列就需要一套策略。我们可以配置这么一套策略，比如可以优先把应用调度到应用明确指定的队列，如果找不到再将其调度到其他队列
+ 资源抢占:某些对执行时间有要求的场景下，我们可以开启资源抢占策略，当一个应用等待一段时间还没有应用释放时，集群直接关闭某些正在运行的应用释放资源。这种方式会影响系统的整体效率
+ 主导资源公平性:应用对资源的请求一般是混合(cpu+内存)，如果两个请求一个需要1cpu + 100G内存，一个需要大量10cpu+1G内存，集群该如何分配资源呢。yarn采用主导资源的概念解决这个问题，即先计算出一个请求的资源请求占用整个集群资源的比重，以占用率大的资源作为主导资源。然后根据请求主导资源的申请量进行调度


# Map/Reduce
## map过程
1. 当client将任务提交到Yarn集群之前，任务所需的一系列准备工作已经做好，会封装成为一个Job提交给RM，后续AM只需要根据Job中的分片信息创建Task即可。
2. map任务是依据分片的，每个数据分片对应一个maptask。分片只是逻辑概念，包含了数据位置，数据长度等信息，并没有真是的文件数据，也不真的对应HDFS中一个文件块，一个分片可能大于一个文件块可能小于与个文件块，但一般情况下分片与文件块是一一对应关心，这样可以使map任务最大程度利用数据本地化。
3. map任务结果都会输出到磁盘，但是每个map任务处理完数据后并不是立即输出到磁盘，而是先写入一个环形缓冲区,然后由一个后台线程定时将缓冲区数据写入磁盘。环形缓冲区默认100M，一旦缓冲区数据到达阈值(默认0.8)，后台溢写线程就开始溢写磁盘，因为环形缓冲区的特殊结构，在数据溢写过程中Map任务还是可以继续写数据到缓冲区，但是如果写入速度过快，缓冲区被填满，Map任务就会阻塞等待。每次数据溢写都会形成一个新的临时文件，但是这些临时文件最终会进行合并，默认10个文件合并成为一个

## shuffle 过程
1. shuffle是数据从map方法到reduce方法阶段的一个简称，期间包含了很多处理，比如map阶段数据的分区，排序，reduce阶段的排序。所以shuffle横跨了map和reduce，也可以说shuffle分为MapShuffle和ReduceShffle
2. suffle阶段存在三大组件
    + 分区器(Partitioner):用于map阶段数据的分区，map阶段文件溢写与合并时使用分区器将数据分区存储，默认使用HashPartitioner;分区器与reduce任务配合，一般一个分区对应一个reduce任务，只有一个reduce任务，分区器并没有什么意义；每个分区不一定是一个文件，也可能保存在同一个文件
    + 排序器(Sorter):用于map阶段的数据排序，map阶段文件溢写和合并阶段都会进行排序，默认使用字典排序;map阶段的排序是为了reduce阶段做准备，如果程序没有reduce阶段，就不需要排序了
    + 局部合并(Combiner):用于map阶段和reduce阶段的数据合并，map阶段文件合并时都会调用，合并器可以使map任务的输出变小，减小reduce任务压力。此函数在reduce阶段文件合并时也会使用到

## reduce 过程
5. 在yarn集群中reduce任务的优先级比map低，一般有5%的map任务开始运行以后reduce任务才可以开始运行，但是两种任务的运行时间肯定是有重合的。
6. 经过了shuffle阶段map任务最终输出了若干排好序的数据文件，这些文件一般都输出在map任务所在节点的本地磁盘，并且将结果地址汇报给AM。reduce任务的在每个map任务完成后都会去复制map的输出结果,这就是reduce任务的复制阶段，这一阶段有多个线程负责
7. 如果Map输出比较小，复制的数据会被保存在reduce缓冲区，否则溢写到磁盘，如果存在combiner函数，溢写是也会先进行聚合。随着复制的数据副本增多，后台线程也会不断将这些文件合并为更大的排好序的文件。默认10个文件合并为一个，最终形成若干个比较大的排好序的文件。这一阶段称为合并阶段
8. 当reduce复制到所有的map任务结果并合并所有文件后进入reduce阶段，将所有文件执行reduce函数，得到最终结果。需要注意的是reduce阶段其实也是最后一次合并，是从磁盘到内存的合并，这样可以减少一次磁盘IO


# map任务分片专题
+ 文件分片是一个很复杂的工作，因为不同类型的文件，不同业务场景需要的分片是不同的，hadoophadoop提供一个专门的接口进行任务分片，并且提供了多个实现类应对多种场景
    - FileInputFormat:所有分片类的父类，包含一个公共的分片方法，但是没有读取数据的方法。分片方法以（文件 + block）为基准进行分片，如果一个目录下有两个小文件，大小都小于一个block块，那么会产生两个map任务；如果文件比较大，一个文件跨越几个hdfs的block，那就会有多个map任务来处理这个文件。所以hadoop不适合处理小文件，因为会产生大量的map任务，产生大量额外开销，此外，小文件也会产生大量随机IO，导致性能非常差
    - TextInputFormat:默认分片类，继承自FileInputFormat，它本身没有分片方法，分片调用父类分片方法。 用于文本文件分片，文本文件一般按行读取，每个分片就是若干行数据。因为行的长度不定，一行数据保存常常跨越多个分块，为了保证一行的完整，此分片类的分片常常不是与hdfs的分块对齐，所以map任务在处理时需要进行一些网络io
    - NLineInputFormat:TextInputFormat 和 KeyValueTextInputFormat 两者每个分片的行数是不固定的，有是需要每个Map任务的行数固定就需要使用 NLineInputFormat 
    - KeyValueTextInputFormat:TextInputFormat在map阶段是一行一行处理的，key是行的字符偏移量，有些时候偏移量并没有什么用，我们想要行中的某些数据作为key,比如日志文件的错误级别或者时间，可以使用 KeyValueTextInputFormat 实现此功能
    - CombineTextInputFormat:将一个文件夹下的文件合并为一个分片，使用一个map任务处理，其内部已经实现了文件的合并，使用时只需要设置文件最大切片大小即可，但是这个类只能处理文本文件
    - SequenceFileInputFormat:也是为了处理小文件的类，可以将多个小文件合并为一个文件，减少map任务数量。此类可以用于合并二进制小文件
    - MultiFileInputFormat:有时同一个map任务有多个数据源，且不同数据源的数据格式不同，需要为每个数据源的指定单独的map算法，此时需要使用 MultiFileInputFormat
    - DBInputFormat:用于map任务从数据库读取数据，需要注意的是运行太多的Map任务从数据库读取数据，数据库压力会很大，所以 DBInputFormat 最好加载少量数据集

# 计数器
hadoop作业在集群中执行时非常难以调试，为了能让我们能更高的了解作业执行情况，hadoop提供了各种计数器，比如此作业运行的map任务个数，reduce任务个数，处理的数据量等等。通过这些计数器我们可以推测任务运行的整体情况。hadoop的计数器分为两种，任务计数器和作业计数器
+ 任务计数器:这种类型的计数器由其管理的任务维护，并定期将计数器值发送到AM。任务计数器的值每次都是完整传输，且只有当任务运行完成，这些计数器的值才是最终值 
+ 作业计数器:这种类型计数器由AM维护，没有网络传输


