# Yarn
# namenode 数据持久化
+ namenode是HDFS的核心，早期namenode是单点的，存在单点故障，所以数据持久化的就是非常必要，否则一旦namenode出问题，内存中的数据全部丢失，整个集群就无法使用且无法恢复
+ namenode数据的持久化依赖两种文件。一种是快照文件(fsimage),一种是写入日志(edit log).在namenode运行过程中，所有的写入操作都会记录在edit log，但是不会同步到fsimage，在namenode重启的时候两者进行合并，形成一个最新的fsimage
+ fsimage + edit log 的持久化方案存在一个很大的缺陷，就是hdfs集群很少重启，这会导致edit log迟迟不与fsimage合并，edit log会变得非常大，一旦系统重启，重启过程会非常长
+ Secondary NameNode 解决了 edit log 长时间不与 fsimage 文件合并的问题。Secondary NameNode 是运行在Namenode节点之外的独立进程，它会定时通知Namenode进行文件合并，并且拉去namenode的两个持久化文件，在本地合并好了以后再发回Namenode替换掉旧的fsimage。

## 资源调度策略
集群的资源是有限的，面对大量的应用请求，资源的调度就成为一大问题，所有人都觉得自己的应用应该优先处理，所以没有最好的调度策略，必须结合具体的场景制定调度策略。Yarn提供了三种调度策略
+ FIFO调度策略:资源调度遵循先来先执行原则，其优点是实现简单，系统吞吐量大，缺点是大作业会持续占用资源，导致小作业产生饥饿
+ 容量调度策略:将集群资源按照队列划分为多个部分，每个队列又可以继续划分层次，job在提交时可以指定自己使用的队列，执行时只能使用本队列的资源
+ 公平调度策略:公平调度策略采用一种动态调度机制，A启动一个作业会先占用集群所有资源，此时B启动一个作业，会占用集群1/2资源，B再启动第二个作业，那么B的两个作业会平分1/2资源
+ 以上三种调度策略可以相互结合，比如先在最顶层使用容量调度策略将集群分为不同的几个队列，每个队列中再使用FIFO调度策略或者公平调度策略
    - 队列放置:如果集群顶层采用公平调度策略将集群分为多个队列，那么多个队列之间的分配是公平的，意味着应用会被公平的调度到这几个队列，那么具体调度到哪个队列就需要一套策略。我们可以配置这么一套策略，比如可以优先把应用调度到应用明确指定的队列，如果找不到再将其调度到其他队列
+ 资源抢占:某些对执行时间有要求的场景下，我们可以开启资源抢占策略，当一个应用等待一段时间还没有应用释放时，集群直接关闭某些正在运行的应用释放资源。这种方式会影响系统的整体效率
+ 主导资源公平性:应用对资源的请求一般是混合(cpu+内存)，如果两个请求一个需要1cpu + 100G内存，一个需要大量10cpu+1G内存，集群该如何分配资源呢。yarn采用主导资源的概念解决这个问题，即先计算出一个请求的资源请求占用整个集群资源的比重，以占用率大的资源作为主导资源。然后根据请求主导资源的申请量进行调度


# Map/Reduce
## map过程
1. 当client将任务提交到Yarn集群之前，任务所需的一系列准备工作已经做好，会封装成为一个Job提交给RM，后续AM只需要根据Job中的分片信息创建Task即可。
2. map任务是依据分片的，每个数据分片对应一个maptask。分片只是逻辑概念，包含了数据位置，数据长度等信息，并没有真实+的文件数据，也不真的对应HDFS中一个文件块，一个分片可能大于一个文件块可能小于与个文件块，但一般情况下分片与文件块是一一对应的，这样可以使map任务最大程度利用数据本地化。
3. map任务结果都会输出到磁盘，但是每个map任务处理完数据后并不是立即输出到磁盘，而是先写入一个环形缓冲区,然后由一个后台线程定时将缓冲区数据写入磁盘。环形缓冲区默认100M，一旦缓冲区数据到达阈值(默认0.8)，后台溢写线程就开始溢写磁盘，因为环形缓冲区的特殊结构，在数据溢写过程中Map任务还是可以继续写数据到缓冲区，但是如果写入速度过快，缓冲区被填满，Map任务就会阻塞等待。每次数据溢写都会形成一个新的临时文件，但是这些临时文件最终会进行合并，默认10个文件合并成为一个

## shuffle 过程
1. shuffle是数据从map方法到reduce方法阶段的一个简称，期间包含了很多处理，比如map阶段数据的分区，排序，reduce阶段的排序。所以shuffle横跨了map和reduce，也可以说shuffle分为MapShuffle和ReduceShffle
2. suffle阶段存在三大组件
    + 分区器(Partitioner):
        - 用于map阶段数据分区，决定map阶段的key被分配到哪个reduce，默认算法是 (key.hashCode() & Integer.MAX_VALUE) % numReduceTask;
        - 正常情况下分区数量与reducer数量一样，处理数据时一一对应，reducer数量可以比分区数量多但是不可以少(除非只有一个reducer);如果只有一个reducer，分区器的数据会到同一个reducer,那么分区器也就没什么作用
    + 排序器(Sorter):
        - 用于map阶段的数据排序，map阶段有两次排序，一是文件溢写二是文件合并，溢写阶段使用快排，合并阶段使用归并排序，默认使用字典排序;
        - map阶段的排序是为了reduce阶段做准备，reducer只需要遍历很少的文件就可以获取到对应分区的所有数据，如果程序没有reduce阶段，就不需要排序了
        - reduce阶段文件溢写时也会进行一次排序，是在reducer从所有map输出文件拉取数据时进行一次归并排序
    + 局部合并(Combiner):用于map阶段和reduce阶段的数据合并，map阶段文件合并时都会调用，合并器可以使map任务的输出变小，减小reduce任务压力。此函数在reduce阶段文件合并时也会使用到

## reduce 过程
5. 在yarn集群中reduce任务的优先级比map低，一般有5%的map任务开始运行以后reduce任务才可以开始运行，但是两种任务的运行时间肯定是有重合的。
6. 经过了shuffle阶段map任务最终输出了若干排好序的数据文件，这些文件一般都输出在map任务所在节点的本地磁盘，并且将结果地址汇报给AM。reduce任务的在每个map任务完成后都会去复制map的输出结果,这就是reduce任务的复制阶段，这一阶段有多个线程负责
7. 如果Map输出比较小，复制的数据会被保存在reduce缓冲区，否则溢写到磁盘，如果存在combiner函数，溢写是也会先进行聚合。随着复制的数据副本增多，后台线程也会不断将这些文件合并为更大的排好序的文件。默认10个文件合并为一个，最终形成若干个比较大的排好序的文件。这一阶段称为合并阶段
8. 当reduce复制到所有的map任务结果并合并所有文件后进入reduce阶段，将所有文件执行reduce函数，得到最终结果。需要注意的是reduce阶段其实也是最后一次合并，是从磁盘到内存的合并，这样可以减少一次磁盘IO


# map任务分片专题
+ 文件分片是一个很复杂的工作，因为不同类型的文件，不同业务场景需要的分片是不同的，hadoophadoop提供一个专门的接口进行任务分片，并且提供了多个实现类应对多种场景
    - FileInputFormat:所有分片类的父类，包含一个公共的分片方法，但是没有读取数据的方法。分片方法以（文件 + block）为基准进行分片，如果一个目录下有两个小文件，大小都小于一个block块，那么会产生两个map任务；如果文件比较大，一个文件跨越几个hdfs的block，那就会有多个map任务来处理这个文件。所以hadoop不适合处理小文件，因为会产生大量的map任务，产生大量额外开销，此外，小文件也会产生大量随机IO，导致性能非常差
    - TextInputFormat:默认分片类，继承自FileInputFormat，它本身没有分片方法，分片调用父类分片方法。用于文本文件分片，文本文件一般按行读取，每个分片就是若干行数据。因为行的长度不定，一行数据保存常常跨越多个分块，为了保证一行的完整，此分片类的分片常常不是与hdfs的分块对齐，所以map任务在处理时需要进行一些网络io
    - NLineInputFormat:TextInputFormat 和 KeyValueTextInputFormat 两者每个分片的行数是不固定的，有是需要每个Map任务的行数固定就需要使用 NLineInputFormat 
    - KeyValueTextInputFormat:TextInputFormat在map阶段是一行一行处理的，key是行的字符偏移量，有些时候偏移量并没有什么用，我们想要行中的某些数据作为key,比如日志文件的错误级别或者时间，可以使用 KeyValueTextInputFormat 实现此功能
    - CombineTextInputFormat:将一个文件夹下的文件合并为一个分片，使用一个map任务处理，其内部已经实现了文件的合并，使用时只需要设置文件最大切片大小即可，但是这个类只能处理文本文件
    - SequenceFileInputFormat:也是为了处理小文件的类，可以将多个小文件合并为一个文件，减少map任务数量。此类可以用于合并二进制小文件
    - MultiFileInputFormat:有时同一个map任务有多个数据源，且不同数据源的数据格式不同，需要为每个数据源的指定单独的map算法，此时需要使用 MultiFileInputFormat
    - DBInputFormat:用于map任务从数据库读取数据，需要注意的是运行太多的Map任务从数据库读取数据，数据库压力会很大，所以 DBInputFormat 最好加载少量数据集

# Map任务个数（参考：https://uzzz.org/2019/04/07/728331.html）
+ 每个Job的map任务个数和分片相关，每个分片对应一个map任务，分片并不是hdfs的文件块，而是一个逻辑概念，表述一个maptask处理的数据。分片的数量由goalSize, minSize, blockSize这三个值决定:splitSize = max(minSize, min(goalSize, blockSize))
    - goalSize: 表示期望每个maptask处理的数据量。goalSize = totalSize / mapred.map.tasks
    - minSize: 每个maptask处理的最小的数据量，minSize = max(1,mapred.min.split.size)
    - blockSize : hdfs的文件块大小，默认为128M，可以通过参数dfs.block.size设置
+ 计算出了splitSize后就可以计算Map任务的数量:final_map_num = min(compute_map_num, input_file_num)
    - default_num = total_size / block_size
    - goal_num =mapred.map.tasks
    - split_num = total_size / max(mapred.min.split.size,block_size)
    - compute_map_num = min(split_num, max(default_num, goal_num))
+ 经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：
    - 如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。
    - 如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。

# 计数器
hadoop作业在集群中执行时非常难以调试，为了能让我们能更高的了解作业执行情况，hadoop提供了各种计数器，比如此作业运行的map任务个数，reduce任务个数，处理的数据量等等。通过这些计数器我们可以推测任务运行的整体情况。hadoop的计数器分为两种，任务计数器和作业计数器
+ 任务计数器:这种类型的计数器由其管理的任务维护，并定期将计数器值发送到AM。任务计数器的值每次都是完整传输，且只有当任务运行完成，这些计数器的值才是最终值 
+ 作业计数器:这种类型计数器由AM维护，没有网络传输

# 推测执行
+ mapreduce的一个缺点就是木桶效应，决定程序整体执行时间的往往是执行最慢的任务，机器老化或者节点带宽不够往往造成任务执行缓慢，但是又总能执行成功。为了解决这类难以排查的问题，hadoop提供执行推测机制，监控所有任务的执行进度，并计算出同类任务执行的平均时间，对于那些慢于平均时间的任务，hadoop会启动备份任务，原任务或者备份任务任意一个执行成功，该任务就算成功，未完成的任务直接丢弃


# hadoop实现Join
+ reduce side join:最简单join方式，将两个来源的key相同的数据分配到一个reduce,reduce程序记录下所有的key进行连接。因为key要保存在内存很容易出现内存溢出，且不能利用分布式的优势
+ map side join：每个map任务都在内存中保存一份来自一个来源的数据，然后在map计算时和另一个来源的数据进行连接。此方法的缺点是两方数据必须是一个大表一个小表
+ SemiJoin:半连接，如果两个表并不是所有的key都需要连接，从小表先抽出需要连接的key保存在内存,然后在map阶段去除两个表中不需要连接的数据，最后在reduce端连接。这种方式可以大大减少网络传输和数据量，但是只适合部分连接。从小表抽取的key集合一般比较小，但是如果内存依旧无法保存，可以使用布隆过滤器进一步缩小这个集合

# MapReduce框架中的分布式缓存
+ mapreduce 提供了一个分布式缓存机制用于提升任务计算速度，或者实现一些内存连接的操作，我们可以选择将的一些文件存放在hadoop的分布式缓存中共享，在job执行前，AM会将共享文件先分发到所有将要执行task节点
+ 分布式缓存的好处是比直接使用hdfs作为共享文件的载体更高效，一般用于共享一些外部字典文件(比如黑白名单),大小表连接，共享一些jar包之类的

# mapreduce 数据倾斜
+ mapreduce计算过程中的数据倾斜主要发生在reduce阶段，生产计算常常是按照key进行分组，每个key对应一个reduce进行聚合计算，那就很容易出现key值数量分布不均匀，导致部分reduce执行非常缓慢
+ 数据倾斜 常常出现在表的聚合和连接，表聚合使用 groupby函数，某个值数量过多导致数据倾斜;两个表进行join时，某些key数量比较比较多，导致某些reduce任务处理的数据量很大;或者数据分桶存储时，分桶字段的空值记录很多，导致他们都往一个桶存储
+ 解决数据倾斜围绕两个指导思想:
    - 尽可能使key均匀:1）调整key让其更均匀，比如可以把key=aaa 的数据加上a,b,c,d 的后缀，让其分布在不同的任务中，最后再进行合并 2）调整map策略，进行两次reduce操作，第一次key随机分配，然后进行聚合计算，可能相同的key被分配到不同的reduce,第二次再将上一步聚合好的数据按照key再次聚合
    - 提前进行局部聚合:1）能在map端聚合的数据就先聚合好 2）join时可以使用map join

# 优化
## 数据存储阶段
+ 避免存储小文件，普通的文本文件就进行合并后再存储，二进制文件可以使用SequenceFile合并。如果集群中已经存在了很多小文件，使用hadoop archive进行归档 
+ 使用压缩文件，压缩文件可以让hdfs集群存储更多内容，也减少了很多网络传输，但是要注意使用的压缩算法是否支持分片

## 数据计算阶段
### map
+ 针对小文件，使用 CombineFileInputFormat 合并小文件之后计算
+ 针对数据倾斜，尽可能让key分布均匀，在数据存储时或者在map计算时使用特殊手段，或者在计算时进行局部合并
+ 针对能进行局部合并的数据，先进行combine处理
+ 调整map task 最大资源限度，默认每个map task对多占用一个cpu和1G内存，reduce task也是一样 
+ 开启推测执行

### shuffle
+ 调整环形缓冲区大小或者临界比例，提高触发溢写的临界值，减少溢写减少IO
+ 增大文件合并标准，减少临时文件合并次数
+ 进行数据压缩，可以配置mapreduce的中间结果压缩，减少网络传输

### reduce
+ 避免使用reduce,如果结果不需要reduce,就不要设置reduce
+ 合理分配map数量和reduce数量，让两者都尽量不出现等待
+ 调整reduce获取map数据的并行度，默认是5，可以根据实际情况适当调整
+ 调整reduce缓冲区大小，在数据量比较小的情况下数据不要溢写磁盘，直接从内存进入reduce计算

### 参数调优
+ dfs.namenode.handler.count 配置namenode处理请求线程池大小


# 数据压缩
+ Hadoop的数据压缩分为HDFS源文件数据压缩和map/reduce计算的数据压缩，前者的主要目的是为了节省存储空间，后者的主要目的是为了减少网络传输开销
+ 针对HDFS文件的压缩需要考虑任务分片问题，因为在map/reduce中一个分片开始一个map任务，但是某些压缩算法压缩的数据并不支持分片，比如zip,这种压缩的数据在计算时只能将多个block放在一个map任务中计算，失去了数据本地性优势增大了网络开销，而且会导致一个map任务处理的数据过多。当然，如果能在数据源中提前处理数据，将数据压缩后保存在hdfs,保证每个压缩文件和一个block差不多大，就可以不必考虑分片问题
+ 针对map/reduce计算的数据压缩，hadoop提供了多种压缩算法，有压缩率高的，压缩解压速度快的，是否支持分片的，可以根据业务需求配置不同的压缩算法。map/reduce计算的压缩主要用于减小map任务输出，在进行shuffle时可以减少网络开销
+ Sequencefile也支持压缩， Sequencefile是hadoop提供的一种特殊的文件格式，可以存储二进制文件也可以作为小文件容器，此文件格式本身就支持压缩，只要开启压缩配置即可，而且此压缩文件支持分片