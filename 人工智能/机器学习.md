# 基本术语
+ 机器学习的数据集合称为数据集 (data set)
+ 每条数据为一个示例或样本 （instance）
+ 示例结果信息被称为标记（label）
+ 拥有标记的示例被称为样例（example）
+ 反映事物在某方面表现或者性质称为属性(attribute) 或者特征 (featue);
+ 属性张成的空间被称为属性空间 （attribute space）或样本空间（sample space）例如判断西瓜是否熟了，把西瓜的大小，颜色，敲击声响作为三个坐标张成一个描述西瓜的三维空间，每个西瓜都会落在这个空间的一个坐标，每个坐标点都称为一个特征向量（feature vector）
+ 学得模型对应的关于数据的某种潜在规律称为假设（hypothesis），潜在规律自身称为真实，学习目的就是为了逼近真实。
+ 假设是潜在规律，潜在规律无从把我，机器学习的过程就是在所有可能的假设中进行搜索，找到一个最匹配训练集的假设，这是一个归纳过程，称为归纳学习(inductive learning)
+ 好的假设可以很好的匹配训练集，甚至训练集之外的数据集，但假设并非真理，它可能存在或不存在，可能唯一或不唯一，在训练过程中可能找到了多个假设都能很好的匹配当前训练集，那么学习过程就产生了一个假设集合，我们称之版本空间(version space)
+ 版本空间会产生一个问题，可能多个假设对数据集的整体匹配正确率一致，但是对单个用例匹配结果不一致，模型必须对版本空间的假设进行偏好选择， 称为归纳偏好(inductive bias)
+ 预测值是离散值，例如好瓜，坏瓜，此类学习任务被称为分类（classification）；若预测值为连续值，如西瓜的成熟度为0.75、0.97，此类学习任务被称为回归(regression)
+ 学得模型后对其进行预测的过程称为测试，被预测的样本称为测试样本
+ 还可以对样本进行聚类(clustering), 即将训练集的数据分为若干组，每组称为一个簇（cluster）
+ 根据训练数据是否拥有标记信息，学习任务可以分为监督学习(supervised learning)和无监督学习(unsupervised learning)，分类和回归属于前者，聚类属于后者
+ 学得模型适应新样本的能力称为泛化能力(generalization)

# 机器学习模型训练步骤
1. 获取数据
2. 数据基本处理
3. 特征工程: 把数据集变为对机器更友好的数据
    + 特征提取
        - 基于信息熵和信息增益选择合适的特征
    + 特征预处理
        - 归一化
        - 标准化
    + 特征降维(去除冗余的，重复的特征)
        - Filter
            - 方差选择法: 过滤低方差特征，方差低说明这个特征所有样本都差不多
            - 相关系数(皮尔森相关系数、PCA降维度): 过滤掉特征之间相关性很强的特征，相关性很强说明特征重复或者冗余了
        - Embeded
            - 决策树
            - 正则化
            - 深度学习
4. 机器学习算法模型训练
    + 机器学习的实质就是从在输入空间和输出空间之间找到一个映射关系，这个映射关系可以很好的匹配和预测输入空间和输出空间，这个映射关系的具体体现就是模型
        - 输入空间: 输入的特征的全集
        - 输出空间: 输出的特征全集
        - 假设空间: 输入空间和输出空间的映射关系的全集， 这个映射关系可能非常多，机器学习的目的就是找到一个能最好的表示两者映射关系的集作为最终的模型输出
    + 梯度下降
        - 梯度下降算法可用用于众多模型的训练，它可以帮助我们一步步逼近最优模型
            - 以线性回归模型为例，其目的就是为了找到一个公式能很好的匹配训练集, 假设有一个单特征值的训练集，公式的一般形式就是 y = wx + b，那么模型的最终目的就是找到合适的 w 和 b 让 y 能尽可能接近真实的目标值。每训练出一个模型就计算一遍y和真实值的差异w，不同的x和b产生的最终的差异肯定是不同的，一开始我们并不知道w 和 b 取多少合适，也不知道w应该怎么变化，这时就需要使用梯度下降算法。
        - GD , SGD , SAG
    + 过拟合
        - 过拟合就是模型训练出极其复杂的函数适配训练集，过于完美的适配训练集反而导致模型在新用例上适配不好
        - 如果模型的损失函数值在训练集和测试集上的差别越来越大说明已经开始过拟合
        - 解决过拟合的三种方法
            1. 增加更多数据集
            2. 缩减特征选择
            3. 正则化
    + 正则化 (https://cloud.tencent.com/developer/article/1379716)
        - 正则化是解决模型过拟合的一种方式，其实现就是在损失函数后加一个惩罚项函数，使损失函数的目标发生了改变。原来训练模型是以最小化损失函数为目标，变成以最小化损失和复杂度为目标
        - 常用的正则化有两种
            - L1：曼哈顿距离（参数绝对求和）: LASSO回归
            - L2：欧氏距离（参数平方值求和）: 岭回归
5. 模型评估
    + 模型评估方法
        - 留出法: 把训练集留出一部分作为测试集
        - 交叉验证: 把训练集划分为N份，把每一份分别当做测试集，其他作为训练集
    + 性能度量
        - 错误率
        - 精度
        - 查准率
        - 查全率
        - 损失函数
    + 模型选择
        - 模型复杂度选择: 一个模型的多项式复杂度越高模型越复杂，对数据集的拟合度也会越高，但模型并非越复杂越好。为了评估出模型到底应该使用多复杂的多项式，需要在综合考虑训练集的损失函数结果和测试集的损失函数结果。随着多项式的维度越多，训练集的损失函数结果会越来越小，但是测试集的损失函数结果则会表现出一种U形走势，两者相距最近的点就是最合适的模型
        - 模型训练中常常需要评估多个模型好不好，一般数据集分为训练集，交叉验证集，测试集三部分。因为如果只有训练集和测试集两部分，训练就相当于一直针对测试集在进行调整，即便最终测试指标很好，那也可能是模型对测试集过拟合，对新增用例的泛化效果并不好
6. 应用
# 机器学习算法分类
+ 监督学习: 数据集包含特征值和目标值
    - 分类问题: 目标值是离散的
        - K-近邻算法
            - 算法的核心就是以邻居的类型判断我的类型，我的类型就是距离我最近的K个邻居的类型
            - 判断两个数据的距离可以选择欧式距离法或者曼哈顿距离等；K 的取值需要根据训练出的模型进行评估
        - 贝叶斯分类
        - 决策树与随机森林
        - 逻辑回归
    - 回归问题: 目标值是连续的
        - 线性回归
        - 岭回归
+ 无监督学习: 数据集只包含特征值，不包含目标值
    - 聚类k-means

# 机器学习方法三要素
+ 模型:输入空间与输出空间之前的映射关系，学习过程就是从假设空间中找到一个和当前训练集拟合最好的映射关系
+ 策略:从假设空间中选中最合适的策略的学习标准或规则
    - 要解决的问题
        1. 评估某个模型对单个样本的效果
        2. 评估某个模型对训练集的整体效果
        3. 评估某个模型对训练集和测试集的整体效果
    - 常用评估指标
        1. 损失函数:0-1损失函数、平方损失函数、绝对损失函数
        2. 风险函数: 
            - 经验风险: 损失函数衡量模型对单个样本的预测结果，那么模型针对训练集预测结果被称为经验风险，求取训练集每个样本的损失函数结果，累加值可以得到经验风险。值越小说明此模型对训练集拟合度越高
            - 期望风险: 模型针对所有集，包括训练集和预测集的预测值。我们的目标就是选择一个可以让期望风险最小的模型
            - 结构风险: 经验风险越小说明模型对当前训练集的拟合度好，但并不是越小越好，模型与训练集拟合度太高，反而会导致期望风险变大，这就是过拟合。可以利用结构风险函数来限制这种过拟合的情况
+ 算法:模型的具体算法