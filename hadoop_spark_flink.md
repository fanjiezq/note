# 作业调度
+ hadoop严格按照map/reduce模式，在map阶段将数据切分成片封装成mapTask，然后分发给集群node运行，然后根据业务需求启动若干ReduceTask完成计算输出结果；在map/reduce过程中，必然经过Shuffle阶段，特别是存在多个ReduceTask的情况下，Shuffle阶段将数据数据经过分区，排序，局部合并，保证数据能正确高效的进入ReduseTask
+ Spark将运行流程划分为多个stage,每个stage又分为多个task在集群中的多个node运行，每个stage的运算结果交给下一个stage继续处理。可以看出Spark的运行流程其实就是多个map/reduce连接起来的，除了第一个和最后一个stage,其他的stage即是map任务也是reduce任务。 这种将多个map/reduce阶段连接成逻辑链的方式让数据处理更灵活了，可以非常简单的在一个程序中编写多个map/reduce逻辑。
+ flink的任务调度依赖 ExecutionGraph，在程序运行初期根据算子的关系构建出运行图，然后启动相应的TaskManager执行运算，可以说在ExecutionGraph构建完成后每个元素的数据流路径已经确定，每个元素会经过固定的数据流路线到经过各个TaskManager执行算子的运算

# shuffle过程
+ hadoop的shuffle阶段比较复杂，包含了数据的分组，排序，局部合并，其流程固定，是map/reduce最主要的性能优化点
+ spark在stage之间存在shuffle,从高维度上看此阶段和hadoop一样，但是其底层的实现方式不同，spark的shuffle更多的是确定上游的数据该如何分发到下游，没有排序(后来也默认排序，但是也可配置为Hash  shuffle)，局部合并等流程，需要依靠算子自行实现
+ flink没有shuffle概念，取而代之的是partitioning，即上游元素如何分发到下游，与spark的不同点在于，spark上游数据所有的数据准备好以后才可以分发到下游，有一个阻塞的过程，flink则是一个元素一个元素的处理，所以它不需要复杂混洗流程，只需要一个分发策略


# 并行度
三个框架都是分布式计算框架， 核心思想都是分而治之，用并行计算提升计算的整体效率，但是三者在实现并行度策略上有所不同
## hadoop
+ mapreduce的核心功能是海量的数据计算，因为其处理的数据量巨大，所以必须限制每个任务的数据量，不能让一个map任务处理过多的数据，否则无法充分发挥分布式计算的能力，拉长计算时间，所以hadoop分区基于数据
+ mapreduce的并行读依靠文件切片，如果文件比较小，一个文件就是一个切片，如果文件比较大，一个切片一般为一个文件块，每个切片对应一个map任务

## Spark
+ spark的job是分阶段的，且阶段之间存在顺序依赖，所以每个阶段都类似与一次mapreduce,可以使用集群的几乎所有计算资源，如何让这些计算资源充分利用提升效率是spark考虑的核心，所以spark的每个阶段都可以设置并行度，一般设置为可使用资源的cpu核心数，可以最大程度利用资源
+ spark的第一个阶段是直接对接数据源，无法设置并行度，它根据数据源的不同实现不同的并行度，如果数据源是HDFS的话，并行度就按照hdfs的标准，等于处理的文件块数;如果是本地文件，

## flink
+ Flink程序一般使用各种算子将数据进行一系列的转换和分发，在实际运行过程中，每个算子一般对应一个任务，而这些任务又可以被分为若干子任务，子任务的数量就是这个任务的并行度，每个算子的并行度都是可以设置的
+ 任务与任务之间存在依赖关系，上游任务处理完成数据以后，会立即将数据传递到下游，而且数据的传递并不需要等待。flink不同任务之间的并行度是各不相关的，每个任务都可以设置不同的并行度，那么就存在一个数据分配方式的问题
+ flink的任务之间数据传递分为两种:
    - 一对一模式:两个任务的并行度相同，且不存在需要重新分发数据的算子，则上游算子的每个子任务与下游算子的每个子任务一一对应，数据的传递也是一一对应
    - 重新分发模式:上下游两个任务的并行度不同，或者因为某些算子需要将数据按照一定规则分配(keyBy)，则上游算子的数据需要被打乱分发到下游算子的各个子任务
+ flink的每个子任务运行都需要一个slot,父任务相同的子任务不能运行在同一个slot,但是父任务不同的子任务可以共享slot，所以想要运行一个flinl程序，集群slot数量等于算子的最大并行度即可

# 数据倾斜
## 产生位置
+ hadoop 的数据倾斜只会发生在shuffle阶段，存在多个分区的时候大量数据被分在少量分区，导致少量reducer处理大量数据，任务总体时间被少量任务拉长
+ spark 的数据倾斜也是只发生在shuffle阶段，存在reduceByKey， groupByKey，join等触发shuffle算子的位置
+ flink 的数据倾斜一般出现在两个位置，数据源（比如kafka的分区数据本身倾斜），keyBy等分流的算子
## 解决办法
数据倾斜的解决办法思想大致相似， 最根本的思想就是让数据均匀分布，从解决的源头分可以分为两类 1）在数据源解决 2）在数据处理过程中解决 ；从实现方式也可以分为两类 1）针对key解决 2）针对计算流程解决
+ 从数据源头解决数据倾斜
    - hadoop和spark一般对接hdfs或者hive，如果数据本身倾斜，部分类型数据很多，部分很少，那么可以尝试对数据进行预处理，提前聚合，或者在程序录入数据时聚合（这是最理想的情况），这种方式保证了数据处理时不存在倾斜的情况，但是其本质只是把对倾斜数据的处理提前了，并没有根本的性的解决
+ 在数据处理过程中解决数据倾斜
    - 因为数据倾斜的场景很多，比如聚合操作某几个key数据很多，大量的key数据很多，join操作某几个key数量很多。不同的场景解决方式不同，但是都属于在数据处理过程中解决数据倾斜，具体见下文
+ 针对key解决数据倾斜
    - 如果进行聚合或者join操作发现只有非常少量的key的数据量远高于平均值，可以先过滤这几个key，然后单独提取出这几个key进行处理，如果
    - 如果进行聚合操作，且有很多key的数据量远高于平均值，可以使用两阶段聚合，首先将key加上不同前缀，分批聚合，然后第二阶段去除前缀，全量聚合
+ 针对计算流程解决数据倾斜
    - 针对join操作，且两个数据集一大一小，小的足以存储在内存中，考虑使用map端join，将小的数据集放在mapper的内存，将大数据集拆分
    - 如果join操作的两个数据集都很大，且有很多key的数据量远高于平均值，采用类似两阶段聚合，将把key加上不同的后缀进行局部连接，然后去除后缀进行全局连接


# 流处理(spark vs flink）
## 模型对比
+ Spark的DStream是微批的方式模拟流，其本质依旧是批处理，就是将一个个小的时间间隔的数据封装成一个个RDD计算;Structd Stream则是将数据看作一个无边界的表，每到来一个数据表就新增一行，比起DStream这种模型操作更灵活，可以使用SQL的API，而且抛弃了微批的概念，数据可以做到更低的延迟
+ Flink是真正的基于流思想，每来一个数据就处理一个数据，计算没有延迟，可以达到真正的实时计算

## 窗口类型
窗口是为了处理无界的流创建出来的概念，其本质就是一个数据集，一般是时间窗口但也不是必须以时间维度建立窗口，也可以以元素的数量为维度。
+ DStream的窗口就是多个微批，把多个采集周期的数据看作一个数据集合就是一个窗口，因为采集周期的时间是固定且完整不可切分的，所以DStream窗口大小必须是采集周期的整数倍
+ Structd Stream 的时间窗口是基于事件时间的，没有采集周期的限制，指定起来更灵活，可以随意指定窗口大小，非常灵活。而且在Structd Stream认为时间窗口本质就是根据时间这一属性为维度进行聚合操作，时间与其他属性并无区别，所以所有的窗口计算都是一个聚合操作，只是把时间属性作为聚合key
+ flink包含两种窗口，TimeWindow 和 CountWindow，其TimeWindow 的时间语义更丰富，包含 到达时间，处理时间，事件时间，可以根据业务需要指定需要那种时间类型。与Structd Stream不同，时间在flink中地位超然，并不与聚合算子合并，所以每个窗口之后必定跟随一个聚合算子。

## 触发器
+ 触发器就是出发一个窗口执行计算的条件，DStream没有触发器概念，因为它是自动计算的，时间周期一到立即计算
+ Structd Stream 包含四种触发器
    - 动态微批:默认触发器，采用微批模式，计算引擎会尽可能快的采集数据并处理，数据来的比较慢的时候每个元素都会触发一次计算。每一个批处理都在上一次计算结束以后开始
    - 固定微批:指定批处理时间，计算引擎会按照指定的时间进行数据采集并计算，如果上一批的数据在一个时间周期内计算完成，数据引擎也会等待上一周期结束才开始下一周期;如果上一周期计算没有完成，下一周期会等待上一周期完成才开始;如果没有新数据可用，则不会启动微批处理
    - 一次性批处理:仅执行一个微批处理来处理所有可用数据，然后自行停止。适用于一次性作业
    - 连续处理:利用checkpoint机制记录查询状态，可以实现低至1ms的延迟，几乎达到实时计算
+ flink的的触发器比较灵活，可以根据元素触发，根据处理时间触发，根据事件时间触发。默认是根据事件时间触发，元素的时间事件到达时间窗口的结束时间立即触发计算

## 针对迟到数据
+ Structed Stream 依靠 Watermark 机制处理迟到的事件，为事件戳属性绑定一个延迟事件作为watermark，窗口 以 watermark 作为触发计算和关闭窗口的标准，而不是以事件时间作为标准
+ Flink 提供三种机制处理迟到数据
    - WaterMark:与 spark 的 Watermark一样 
    - allowedLateness:为窗口设置一个延迟关闭的时间，为处理那些在窗口正常关闭事件之后一小段时间到来的数据。这类数据每来一个都会触发一次窗口计算
    - sideOutputLateData:为窗口设置一个旁路输出，保存那些在窗口关闭以后到来的数据，后续手动处理

## 状态机制
+ spark提供了一个 mapGroupsWithState 算子用于保存状态，在 流处理过程中可以实现状态的获取和更新，但是此函数必须跟随在groupByKey算子后面
+ flink 提供 RichFunction 接口，实现此借口就可以创建状态变量，并进行访问和更新，所以每个算子内部都可以获取状态。flink提供了五种状态变量(ValueState，ListState，ReducingState，AggregatingState，MapState)可以根据业务场景选择使用。

## 持久化机制和故障恢复
+ 数据持久化有两个作用，一是当程序中有需要复用的计算结果，将其持久化到内存中，达到数据的复用;二是将计算结果和状态持久化到分布式的存储系统，程序进行异常恢复时可以从失败处恢复，不需要重新计算。
    - 针对数据复用,DStream 采用RDD的持久化机制，调用 persist() 就可以将流计算的数据持久化到内存，达到数据复用;对于基于窗口的操作比如reduceByWindow、reduceByKeyAndWindow，以及基于状态的操作，比如updateStateByKey，默认就隐式开启了持久化机制，不需要显示调用persist()方法。
    - 针对异常恢复，Dstream利用checkpoint机制，将计算的元数据和数据保存在外部存储系统，程序异常恢复时从外部存储系统还原计算状态。使用了有状态的transformation操作——比如updateStateByKey，或者reduceByKeyAndWindow操作都需要checkpoint，所以使用这些算子都需要指定一个外部存储路径。使用时指定一个目录和一个触发周期，计算引擎会自动的周期性进行持久化。
    - 异常恢复分为两种，一种是对数据的异常恢复，当程序启动后计算引擎会自动从持久化目录获取数据恢复，不需要额外处理；对元数据的恢复，比如计算流程，则需要修改程序，从checkpoint目录读取数据，根据数据得到一个Context
+ Structd Stream 的持久化机制和异常恢复机制和DStream一样
+ Flink 也有两种持久化方案，但是在概念上与Spark关注的点不同，概念上也有所差别
    - CheckPoint:CheckPoint是flink程序的快照，此快照包含了算子的状态和数据的偏移量，主要用于异常恢复
    - SavePoint:SavePoint也是系统的快照，其使用和原理和CheckPoint一致，只是CheckPoint的数据在程序不需要时自动删除， SavePoint则永久保存，除非用户手动删除，SavePoint主要用于系统升级
    - Flink可以指定持久化数据的存储位置，可以是内存，文件系统，RocksDB
    - 相比于Spark,flink没有针对数据复用的持久化策略，因为flink是每来一个元素就处理一个元素，不会存在大量的数据处理结果需要缓存，所以没有必要


## 性能调优
+ 性能调优的方式很多，要结合具体的业务场景分析，但是一般都是针对以下四点优化
    - 资源的充分和高效利用。可以申请多少资源，如何分配(每个Executor分配多少内存用于计算，多少内存用于保存持久化数据)
    - 并发调优。数据如何分区，最大化利用框架的并发特性
    - Shuffle调优。如何避免shuffle，不可避免时采用哪种shuffle
    - 解决数据倾斜问题