给这段文字排版
# flume
## taildir_to_kafkaAndHdfs.conf
    # Name the components on this agent
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1 c2

    # source配置
    a1.sources.r1.type = TAILDIR
    a1.sources.r1.filegroups = g1
    a1.sources.r1.filegroups.g1 = /home/fanjie/code/bigdata/logs/.*.log  
    a1.sources.r1.interceptors = i1
    a1.sources.r1.interceptors.i1.type = com.study.flume.interceptor.BigdataInterceptor$Builder
    a1.sources.r1.positionFile = ~/.flume/taildir_position.json

    # sink配置
    a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
    a1.sinks.k1.kafka.topic = view_log
    a1.sinks.k1.kafka.bootstrap.servers = master:9092
    a1.sinks.k1.kafka.flumeBatchSize = 20
    a1.sinks.k1.kafka.producer.acks = 1
    a1.sinks.k1.kafka.producer.linger.ms = 1

    a1.sinks.k2.type = hdfs
    a1.sinks.k2.hdfs.path = hdfs://mycluster/bigdata/viewlog
    a1.sinks.k2.hdfs.round = true
    a1.sinks.k2.hdfs.roundValue = 1
    a1.sinks.k2.hdfs.roundUnit = minute
    a1.sinks.k2.hdfs.useLocalTimeStamp = true
    a1.sinks.k2.hdfs.filePrefix = view
    a1.sinks.k2.hdfs.fileSuffix = .log
    a1.sinks.k2.hdfs.rollInterval = 30
    a1.sinks.k2.hdfs.rollSize = 137438953472
    a1.sinks.k2.hdfs.rollCount = 0
    a1.sinks.k2.hdfs.fileType = DataStream

    # channel 配置
    a1.channels.c1.type = memory
    a1.channels.c1.capacity = 1000
    a1.channels.c1.transactionCapacity = 100

    a1.channels.c2.type = memory
    a1.channels.c2.type = memory
    a1.channels.c2.capacity = 3000
    a1.channels.c2.transactionCapacity = 3000


    # Bind the source and sink to the channel
    a1.sources.r1.channels = c1 c2
    a1.sinks.k1.channel = c1
    a1.sinks.k2.channel = c2

    # 启动命令
    bin/flume-ng agent --conf ./conf/ --conf-file conf/bigdata/taildir_to_kafkaAndHdfs.conf --name a1 -Dflume.root.logger=INFO,Console

# hive
    // 以外表的方式直接管理 hdfs 文件
    create table view_log (
        recored_id string comment '访问记录id',
        user_name map<string,string> comment '用户名',
        user_id map<string,string> comment '用户id',
        timetype map<string,string> comment '时间类型',
        time map<string,string> comment '进入视频或者推出视频时间',
        video_id map<string,string> comment '视频id',
        video_type map<string,string> comment '视频标题',
        vcpublishtime map<string,string> comment '视频发布时间',
        iLength map<string,string> comment '视频长度',
        iAutherId map<string,string> comment '作者id'
    )  row format delimited fields terminated by ','  MAP KEYS TERMINATED BY ':' LOCATION '/bigdata/viewlog';

    // 按照 timetype 分桶保存
    create table view_log_bucket (
        recored_id string comment '访问记录id',
        user_name map<string,string> comment '用户名',
        user_id map<string,string> comment '用户id',
        timetype map<string,string> comment '时间类型',
        time map<string,string> comment '进入视频或者推出视频时间',
        video_id map<string,string> comment '视频id',
        video_type map<string,string> comment '视频标题',
        vcpublishtime map<string,string> comment '视频发布时间',
        iLength map<string,string> comment '视频长度',
        iAutherId map<string,string> comment '作者id'
    )  clustered by (timetype) into 2 buckets row format delimited fields terminated by ',' stored as textfile;

    //保存开始观看的记录
    create table view_log_start(
        recored_id string comment '访问记录id',
        user_name map<string,string> comment '用户名',
        user_id map<string,string> comment '用户id',
        timetype map<string,string> comment '时间类型',
        time map<string,string> comment '进入视频或者推出视频时间',
        video_id map<string,string> comment '视频id',
        video_type map<string,string> comment '视频标题',
        vcpublishtime map<string,string> comment '视频发布时间',
        iLength map<string,string> comment '视频长度',
        iAutherId map<string,string> comment '作者id'
    )  row format delimited fields terminated by ',' stored as textfile;

     //保存开始观看的记录
    create table view_log_end(
        recored_id string comment '访问记录id',
        user_name map<string,string> comment '用户名',
        user_id map<string,string> comment '用户id',
        timetype map<string,string> comment '时间类型',
        time map<string,string> comment '进入视频或者推出视频时间',
        video_id map<string,string> comment '视频id',
        video_type map<string,string> comment '视频标题',
        vcpublishtime map<string,string> comment '视频发布时间',
        iLength map<string,string> comment '视频长度',
        iAutherId map<string,string> comment '作者id'
    )  row format delimited fields terminated by ',' stored as textfile;

# Hbase
create 'view_log',{NAME=> 'userinfo'},{NAME=> 'videoifo'}



# 目标
## 分析一个用户最喜欢哪个类型的视频
1. hadoop解决方案
    + 使用map任务计算每个用户观看的每个类型的视频的数量，输出<userId + videType,count>
    + reduce 按照用户id和 videType 一级排序，然后根据的count倒序排序
2. hive解决方案
    + select cast(user_id["id"] as int) as userId,video_type["vcType"] as videoType ,count(*) as viewCount from view_log_start group by user_id, video_type order by userId ,viewCount desc ,videoType;
4. spark解决方案
    + 分析一个
5. flink结局方案
6. hbase解决方案

## 分析每个类型的视频哪个年龄段的观看人员次数排序
1. hadoop解决方案
2. hive解决方案
3. hbase解决方案
4. spark解决方案
5. flink结局方案

## 根据用户前十分钟看的视频，给他推荐相同的类型的视频
1. hadoop解决方案
2. hive解决方案
3. hbase解决方案
4. spark解决方案
5. flink结局方案

## 分析每个类型的视频平均有效播放时常比例(即每个视频用户观看时长/视频总时常 求平均数)