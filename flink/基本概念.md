# 基本架构
+ client: 客户端负责将作业提交给 JobManager，客户端会根据程序逻辑构建一个JobGraph然后提交给JobManager
+ JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：
    - ResourceManager:负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。
    - Dispatcher: 负责接受客户端提交的作业，并为每个作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。
    - JobMaster:负责管理单个JobGraph的执行，JobMaster会接收到一个的 JobGraph 会将其转换为 ExecutionGraph，最终调度到各个TaskManager。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster
+ TaskManager:Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行了。在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据

# 任务槽(Slot)
+ TaskManager是集群中真正负责计算的角色,每个TaskManager都是一个JVM进程,它可能存在一个或者多个线程上执行任务.每个Task Manager 上可运行多少个Executor由插槽决定
+ 每个任务槽代表一组资源, 例如如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存
+ 可以在任务槽中运行一个或多个线程,同一插槽中的线程共享相同的 JVM, 同一 JVM 中的任务共享 TCP 连接和心跳消息
+ Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离
+ 默认情况下，Flink 允许子任务共享 Slot，即使它们是不同任务的子任务，只要它们来自相同的 job,以提高资源利用率，但同一个任务的子任务必须在不同的Slot

# 并行度(parallelism)
+ Flink程序一般使用各种算子将数据进行一系列的转换和分发，在实际运行过程中，每个算子一般对应一个任务，而这些任务又可以被分为若干子任务，子任务的数量就是这个任务的并行度
+ 任务与任务之间存在依赖关系，上游任务处理完成数据以后，会立即将数据传递到下游，而且数据的传递并不需要等待。flink不同任务之间的并行度是各不相关的，每个任务都可以设置不同的并行度，那么就存在一个数据分配方式的问题
+ flink的任务之间数据传递分为两种:
    - 一对一模式:两个任务的并行度相同，且不存在需要重新分发数据的算子，则上游算子的每个子任务与下游算子的每个子任务一一对应，数据的传递也是一一对应
    - 重新分发模式:上下游两个任务的并行度不同，或者因为某些算子需要将数据按照一定规则分配(keyBy)，则上游算子的数据需要被打乱分发到下游算子的各个子任务
+ flink程序算子的子任务会在单独的线程中运行，并且通常情况下是会在不同的机器上运行。在Flink中每个任务都会占用一个基本的资源单元Slot来运行
+ Slot 是指 Taskmanager 的最大并发执行能力;parallelism 是指 taskmanager 实际使用的并发能力

# 作业执行流程
+ Flink程序正式计算之前会按照程序逻辑构建几个流程图,然后根据流程图进行任务调度, StrameGraph > JobGraph > ExecutionGrap > 物理执行图
    - StrameGraph: 首先根据算子的依赖关系和业务逻辑解析生成StrameGraph，此图是根据算子划分的，所以每个算子都是一个节点
    - JobGraph:对于某些比较简单的算子，如果他们的并行度相同，可以放在一个节点执行减少网络传输(比如filter和map)，于是对StrameGraph优化形成JobGraph
    - ExecutionGraph:JobGraph 依旧属于逻辑上的划分，程序的执行最终要落地到集群的每个节点，一般算子都会设置并发度，不可能一个算子就分配一个节点，所以需要将 JobGraph 进一步细化，每个节点会根据节点的并发度拆分成多个节点，就形成了ExecutionGraph，可以把ExecutionGraph当作并行版本的JobGraph
    - 物理执行图: JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。
+ flink程序在集群中一般是分布式并行执行的，ExecutionGraph都会形成多个task，数据流会被均匀的分配到Task节点运算。也有些情况我们需要某些数据集中在一起，可以使用KeyBy算子进行数据分类，将一般Stream转化为KeydStream，flink会根据key不同将不同的数据分配给下游的算子的子任务，此时Key的数量应该大于等于下游算子设置的并发度，否则下游算子的部分子任务实际上并没有工作
+ 无论是KeydStream还是一般的Stream，流中数据发送到哪个下游的task节点是由flink自行决定。有些情况我们想要控制这一过程让数据按照我们的要求进行分配，flink也提供了这种API，一般称为物理分区。具体方法如下：
    - Steam.global: 上游算子将所有记录发送给下游算子的第一个实例。
    - Steam.broadcast: 上游算子将每一条记录发送给下游算子的所有实例。
    - Steam.forward：只适用于上游算子实例数与下游算子相同时，每个上游算子实例将记录发送给下游算子对应的实例。
    - Steam.shuffle：上游算子对每条记录随机选择一个下游算子进行发送。
    - Steam.rebalance：上游算子通过轮询的方式发送数据。
    - Steam.rescale：当上游和下游算子的实例数为 n 或 m 时，如果 n < m，则每个上游实例向ceil(m/n)或floor(m/n)个下游实例轮询发送数据；如果 n > m，则 floor(n/m) 或 ceil(n/m) 个上游实例向下游实例轮询发送数据。
    - Steam.partitionCustomer：当上述内置分配方式不满足需求时，用户还可以选择自定义分组方式

# Window
## 时间语义
Flink定义了三类时间:
+ 处理时间（Process Time）数据进入Flink被处理的系统时间（Operator处理数据的系统时间）
+ 摄取时间（Ingestion Time）数据进入Flink的时间，记录被Source节点观察到的系统时间
+ 事件时间（Event Time）数据在数据源产生的时间，一般由事件中的时间戳描述，比如用户日志中的TimeStamp

## 事件窗口
+ 流数据理论是无限的，处理无限的数据对现实来说并没有什么意义，因为数据没有尽头，结果也无法确定。窗口的引入是为了将数据流进行区域划分，相当于将无界流转化为一个个有界流
+ 事件窗口有几个必要的属性
    - 窗口时间区间
        - 窗口类似一个一个桶，时间区间是其编号，所有的时间区间在程序启动时就已经确定了，与数据时间无关，但是具体的window对象并没有创建，而是先计算每个元素属于哪个window，需要就创建
        - 确定窗口的时间区间，只要确定第一个窗口的起始时间即可，flink包含一个算法TimeWindow.getWindowStartWithOffset()
    - 窗口计算函数
        - ReduceFunction:聚合方法，以增量方式聚合数据，每到一个数据就进行一次聚合，实时性更好
        - AggregateFunction:更加普适的聚合方法，以增量方式聚合数据
        - ProcessWindowFunction:全量聚合函数，数据一直缓存在窗口，直到窗口触发才进行聚合，实时性比较差，但数据处理更灵活
        - 增量聚合可以显著减少内存占用率，增量聚合窗口内部只保存一个状态数据，全量聚就不是
    - 窗口触发器
        - 窗口触发器(Trigger)用于触发窗口计算，窗口不能一直等待数据不进行计算，有很多触发的条件，比如每来一个元素触发一次计算，或者某个事件时间到来才触发计算
        - 窗口触发器可以自定义，默认使用事件时间触发器(EventTimeTrigger),watermaker到达触发条件就触发计算
    - 窗口补充函数
        - Evictor:窗口元素移除器，可以窗口计算前或者计算后移除窗口元素;使用这个函数将会导致窗口无法进行任何预聚合操作;而且窗口并不能保证元素的顺序，不能依赖顺序来移除元素
+ 事件窗口分为以下几种
    - 滚动窗口:以一个个紧密排列方式分割数据流的窗口，前一个窗口的结束是后一个窗口的开始，每个窗口左闭右开，好像一个窗口在不断翻滚，是一个特别的滑动窗口
    - 滑动窗口:从第一个窗口的开始时间，按照特定的步长不断向前滑动，窗口之间一般存在重合
    - 会话窗口:以数据到来的间隔时间作为窗口的划分标准，第一个数据到来开启一个窗口，如果后续在规定时间时间间隔内没有数据到来，窗口就会关闭
    - 全局窗口:将整个流的数据采集到一个窗口，窗口没有结束时间，也没有触发条件，所以必须配合Trigger使用
+ 每个窗口中的数据都是流数据的一份拷贝，它只属于本窗口，且跟随窗口的关闭而消失。所以如果窗口的时间区间过长，内存中会长期保留大量数据拷贝;或者滑动窗口重复区间很多，就会导致产生大量重复的数据拷贝


## 迟到事件处理
当基于时间窗口处理流数据会存在一个问题，何时触发窗口的计算，当时间窗口是基于摄取时间或者处理时间建立的，这个问题很好回答，将系统时间作为判断标准，时间窗口到达时间点就触发计算。但是基于事件时间建立的时间窗口，这个问题就并不那么简单，首先事件时间并不是连续的，两个数据之间可能隔几秒钟，也可能隔几分钟，这个很好解决，只要划分好了时间区间，往对应的窗口存放即可。更重要数据到来的顺序和它们时间发生的顺序并不一致，如果一个事件的时间本应该触发窗口的计算，但是实际此事件之前发生的数据因为网络阻塞还没到达，这部分迟到数据会被丢弃，计算结果必然不准确。想要保证数据准确就必须等待，但是又不可能永远等下去，等待多久合适呢。所以Flink提供了众多手段解决数据迟到的问题
### WaterMark
+ WaterMark 是处理迟到事件的最核心的手段，是Flink添加到数据流中的一类特殊元素，此元素的核心属性是一个时间戳 = 当前事件元素的时间 - 延迟时间，WaterMark被插入到每个真实的数据元素后面。触发窗口计算不再基于事件本身时间，而是基于WaterMark
+ WaterMark 的一个语义是WaterMark时间之前的事件都已经到来，所以当 watermark的值到达了触发时间窗口计算时间，窗口就认为所有的数据都已经到来，立即触发计算
+ 为了达到WaterMark时间之前的事件(很可能)都已经到来这一个含义,WaterMark数据在设置的时候必须是递增的,比如延迟时间2s,当前事件A时间为9:05,WaterMark应该是9:03,在事件A之后到来的事件的WaterMark都必须大于等于9:03,如果后续的时间事件大于9:05,WaterMark就递增.否则WaterMark不变
+ 在理想情况下，当watermark元素到达以后，所有watermark时间之前的数据都会到达，但是生产中总是存在一些极端情况数据在Watermark时间到达以后到达，此时窗口已经计算且关闭了。为了能处理这部分迟到数据，我们可以为时间窗口设置一个延时关闭时间allowedLateness()，延迟一段时间关闭窗口，在watermark以后到达的每个数据都会触发一次计算
+ 在更极端的情况下，即便设置了窗口延迟关闭，还是可能存在一些数据在窗口关闭以后到达， 还可以设置侧输出流sideOutputLateData()，采集那些被窗口丢弃的数据

# 状态管理和容错机制
## 算子状态
+ 流计算最好的方式当然是无状态的，算子只包含计算逻辑，这样程序管理将会非常简单，但是生产中有很多场景无法做到无状态，总有一些业务需要统计流计算中的各种数据，所以状态机制必不可少。flink 提供了状态机制，我们可以非常简单的使用这些api实现各种业务。内置的状态数据结构有如下这些 ValueState，ListState，ReducingState，AggregatingState，MapState。每种状态结构都有其适用的场景。

## CheckPoint
+ 在Flink程序处理过程中，总会出现一些意外导致计算从某个位置出错，如果这个计算流程中存在状态，为了再次得到这个状态，必须从头开始再次计算所有数据，这显然是非常浪费的，flink使用检查点来解决这个问题。
+ 检查点(checkPoint)是flink中一种错误恢复手段，就是在数据计算过程中定时保存计算的中间状态和已经计算的数据的偏移量，如果发生意外，程序重启后继续数据的偏移量开始计算获取数据，中间状态也可以从保存的数据中直接获取
+ 检查点的实现利用了分布式快照(Distributed Snapshots)技术，因为flink程序被算子分为了多个阶段，每个算子又被分为多个子任务在集群分布式运行，为了节点异常后数据能够正常恢复，必须产生一个全局一致的快照。flink采用一个称为stream barriers 的特殊元素，在数据流动过程中，JobManager在数据源的位置定时的插入一个stream barrier，所有的算子只要遇到这个元素就立即准备保存当前的状态和已经处理的数据偏移，出现异常重启后，算子从数据偏移的位置继续处理
    - stream barriers 的一个重要的属性是它的版本，当JobManager发起一个checkpoint就会往流中插入一个barrier N,下游子任务接收到barrier N会将这个N包含的所有数据计算成状态进行保存，然后发送到JobManager。JobManager同时维护了一个checkpoint版本表，表中保存流中每个barriers的版本，和这个版本对应的每个算子的状态数据，这样JobManager就拥有了某个barriers版本下，整个集群中所有算子的状态
    - 算子接收到一个上游分区数据的barriers元素时并不会保存数据，而是等到所有的上游分区的同id的 stream barriers 元素到来才进行保存，在这个等待的过程中算子不再继续计算数据，因为会导致多个算子的状态不一致，但是会把上游后续发来的数据进行缓存，等到检查点数据保存完成后继续进行处理
+ 检查点的数据默认保存在 JobManager 的内存，但是也可以修改配置保存到分布式文件系统。尽管可以将数据保存在文件系统，但是一般只有最新的checkpoint数据有用，往期的数据可以删除，至于需要保存多少个版本的历史数据，flink可以配置
+ checkpoint本身可以配置一个超时时间，如果执行checkpoint的时间超过了超时时间，还在进行中的 checkpoint 操作就会被抛弃
+ checkpoint还可以配置一个最小间隔时间，如果值设置为了 5000， 无论 checkpoint 持续时间与间隔是多久，在前一个 checkpoint 完成时的至少五秒后会才开始下一个 checkpoint。这种机制保证程序不会非常频繁的checkpoint，浪费性能。
+ 一个Job中同一时间一般只有一个checkpoint在执行，这保证系统不会因为过多的checkpoint影响计算，但是有时候并行的checkpoint也是有意义的，需要配置。注意当checkpoint的并行配置和最小间隔时间配置不能同时存在
## SavePoint
+ Savepoint 是依据 Flink checkpointing 机制所创建的流作业执行状态的一致镜像，savepoint 和 checkpoint原理上很相似，用法也很相似，但是它们处理不同的场景，很多特性也不相同
+ Checkpoint 的生命周期由 Flink 管理，数据的管理和删除无需用户干预，默认情况下checkpoint会在作业完成或者取消后自动删除(也可以配置为不删除)。savepoint由用户创建，拥有和删除，是用户有意识有计划的进行的数据备份，所以其数据必须在作业停止后继续存在，数据的恢复成本也更高。savepoint常用作系统升级

## State Backend
+ 算子的State可能很小也可能很大，可能不太重要也可能非常重要不能丢失，为了应对多种场景，flink提供了几种不同的位置存储状态数据，这些数据存储位置统一称为State Backend
+ Flink提供了三种内置的State Backend用于我们保存状态数据
    - MemoryStateBackend:状态数据存储在TaskManager内存，checkpoint数据存储在JobManager内存。 优点是速度快;缺点受内存大小限制(TaskManager和JobManager的内存都有限制)，且数据易丢失。适合测试环境
    - FsStateBackend:状态数据存储在TaskManager内存，CheckPoint的状态快照写入到配置的文件系统目录中。优点是状态访问和更新基于内存，速度快;缺点是状态大小受TaskManager内存大小限制
    - RocksDBStateBackend:状态数据存储在 RocksDB 数据库中，checkPoint数据存储在外部的文件系统。优点是存储空间是可扩展的，状态大小基本没有限制，checkPoint还可以使用增量快照;缺点是状态的访问和更新必须经过序列化和反序列化，而且都需要经过磁盘读写，速度比较慢，会降低系统整体吞吐量
    - RocksDBStateBackend的增量快照:不同于产生一个包含所有数据的全量备份，增量快照中只包含自上一次快照完成之后被修改的记录，因此可以显著减少快照完成的耗时，同时增量快照恢复不需要解析 Flink 的统一快照格式来重建本地的 RocksDB 数据表，直接基于增量文件加载数据，cpu的消耗很小。但是增量快照是基于 RocksDB 内部的一种机制实现，这种实现虽然可以让数据以增量形式保存，但是增量数据和以往快照之间可能存在数据重合，快照体积可能会变大，如果系统的网络是瓶颈，增量快照恢复可能会消耗更多时间(需要注意的是，一旦启用了增量快照，网页上展示的 Checkpointed Data Size 只代表增量上传的数据量)
    
# 广播变量
+ Broadcast是一份存储在TaskManager内存中的只读的缓存数据，用于在执行job的过程中需要反复使用的数据，为了达到数据共享，减少运行时内存消耗，我们就用广播变量进行广播