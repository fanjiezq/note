# 核心组件
+ ResuorceManager:负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。
+ Dispatcher：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况
+ JobManager:JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)包含了所有可以并发执行的任务，然后JobManagers向 ResourceManager 申请资源来执行该任务，一旦申请到资源(就是TaskManager上的插槽(slot))，就会将执行图分发到真正运行它们的TaskManager上 。而在运行过程中，JobManager会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManager，其中一个作为 leader，其余的则处于 standby 状态
+ TaskManager:Flink中的工作进程Worker。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行了。在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据

# 基本架构
+ Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager
+ JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：
    - ResourceManager:负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。
    - Dispatcher: 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。
    - JobMaster:负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby

# 任务槽(Slot)
+ TaskManager是集群中真正负责计算的角色,每个TaskManager都是一个JVM进程,它可能存在一个或者多个线程上执行任务.任务执行的并行性由每个 Task Manager 上可用的任务槽决定
+ 每个任务槽代表一组资源, 例如如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存
+ 可以在任务槽中运行一个或多个线程,同一插槽中的线程共享相同的 JVM, 同一 JVM 中的任务共享 TCP 连接和心跳消息
+ Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离
+ 默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job,以提高资源利用率
+ 不同的任务可以共享一个Slot,默人所有的Slot都可以被任意任务共享,也可以通过设置共享组设置哪些任务共享一个组的Slot资源,哪些任务必须分配在不同的Slot


# 并行度(parallelism) 和 任务槽(Slot)
+ Flink程序一般使用各种算子将数据进行一系列的转换和分发，在实际运行过程中，每个算子一般对应一个任务，而这些任务又可以被分为若干子任务，子任务的数量就是这个任务的并行度
+ 任务与任务之间存在依赖关系，上游任务处理完成数据以后，会立即将数据传递到下游，而且数据的传递并不需要等待。flink不同任务之间的并行度是各不相关的，每个任务都可以设置不同的并行度，那么就存在一个数据分配方式的问题
+ flink的任务之间数据传递分为两种:
    - 一对一模式:两个任务的并行度相同，且不存在需要重新分发数据的算子，则上游算子的每个子任务与下游算子的每个子任务一一对应，数据的传递也是一一对应
    - 重新分发模式:上下游两个任务的并行度不同，或者因为某些算子需要将数据按照一定规则分配(keyBy)，则上游算子的数据需要被打乱分发到下游算子的各个子任务
+ flink程序算子的子任务会在单独的线程中运行，并且通常情况下是会在不同的机器上运行。在Flink中每个任务都会占用一个基本的资源单元来运行，这个资源单元被称为Slot，
+ Slot 是一种资源描述，也是Flink中管理资源的基本单位，每个TaskManager都被分为若干Slot，代表这个TaskManager同一时间可以同时执行多少个运算子任务。如果一个TaskManager只有3个Solt,它就只能并行运行三个Task(不考虑Solt共享)
+ 所以Slot 是指 Taskmanager 的最大并发执行能力;parallelism 是指 taskmanager 实际使用的并发能力

# 任务划分
+ Flink程序正式计算之前会经历几个构建阶段, 数据流图 > Job图 > 执行图 > 物理执行图.
+ Flink程序最终会被分成若干计算任务,分配到各个Slot执行,Task的划分很多时候和我们的代码并不一致,某些算子会被合并到一个Task.合并的规则是如果两个算子的并行度相同,且数据传输形式是one-to-one的,那么两个算子就可以合并

# 事件时间
## 时间语义
Flink定义了三类时间:
+ 处理时间（Process Time）数据进入Flink被处理的系统时间（Operator处理数据的系统时间）
+ 摄取时间（Ingestion Time）数据进入Flink的时间，记录被Source节点观察到的系统时间
+ 事件时间（Event Time）数据在数据源产生的时间，一般由事件中的时间戳描述，比如用户日志中的TimeStamp

## 迟到事件处理
当基于时间窗口处理流数据会存在一个问题，何时触发窗口的计算，当时间窗口是基于摄取时间或者处理时间建立的，这个问题很好回答，将系统时间作为判断标准，时间窗口到达时间点就触发计算。但是基于事件时间建立的时间窗口，这个问题就并不那么简单，首先事件时间并不是连续的，两个数据之间可能隔几秒钟，也可能隔几分钟，这个很好解决，只要划分好了时间区间，往对应的窗口存放即可。更重要数据到来的顺序和它们时间发生的顺序并不一致，如果一个时间的时间本应该触发窗口的计算，但是实际此事件之前发生的数据因为网络阻塞还没到达，计算结果必然不准确。想要保证数据准确就必须等待，但是又不可能永远等下去，等待多久合适呢。所以Flink提供了众多手段解决数据迟到的问题
### WaterMark
+ WaterMark 是处理迟到事件的最核心的手段，是Flink添加到数据流中的一类特殊元素，此元素的核心属性是一个时间戳 = 当前事件元素的时间 - 延迟时间，WaterMark被插入到每个真实的数据元素后面
+ WaterMark 的一个语义是WaterMark时间之前的事件(很可能)都已经到来，所以当 watermark的值到达了处罚时间窗口计算时间，窗口就认为所有的数据都已经到来，立即触发计算，关闭窗口
+ 为了达到WaterMark时间之前的事件(很可能)都已经到来这一个含义,WaterMark数据在设置的时候必须是递增的,比如延迟时间2s,当前事件A时间为9:05,WaterMark应该是9:03,在事件A之后到来的事件的WaterMark都必须大于等于9:03,如果后续的时间事件大于9:05,WaterMark就递增.否则WaterMark不变

### 窗口关闭延迟
+ WaterMark 能应对绝大多数数据迟到的情况，但是并不能完全解决，因为窗口的等待时间有限，总是存在一些极端情况数据在Watermark时间到达以后到达，此时窗口已经计算且关闭了。为了能处理这部分数据，我们可以为时间窗口设置一个延时关闭时间allowedLateness()，延迟一段时间关闭窗口，在watermark以后到达的每个数据都会触发一次计算

### 侧输出流
+ 在更极端的情况下，即便设置了了窗口延迟关闭，还是可能存在一些数据在窗口关闭以后到达， 还可以设置侧输出流sideOutputLateData()，采集那些窗口遗漏的数据

# 状态管理
## 算子状态
+ flink的每个算子都可以实现为有状态的，状态数据保存在taskmanager的内存中
## CheckPoint
在Flink程序处理过程中，总会出现一些意外导致计算从某个位置出错，如果这个计算流程中存在状态，为了再次得到这个状态，必须从头开始再次计算所有数据，这显然是非常浪费的，flink使用检查点来解决这个问题
+ 检查点(checkPoint)是flink中一种错误恢复手段，就是在数据计算过程中定时定点的保存计算的中间状态和已经计算的数据的偏移量，如果发生意外，程序重启后继续数据的偏移量开始计算获取数据，中间状态也可以从保存的数据中直接获取
+ 检查点的实现原理很简单，利用了一个称为stream barriers 的特殊元素，在数据流动过程中，JobManager在数据源的位置定时的插入一个特殊的元素，所有的算子只要遇到这个元素就立即准备保存当前的状态和已经处理的数据偏移，出现异常重启后，算子从数据偏移的位置继续处理
    - stream barriers 的一个重要的属性是它的编号，Job的在向流中插入stream barriers元素时会为每个任务都发送一个检查点，因为flink是并行处理的，下游的算子从不同的分区流接受到多个上游到来的 stream barriers，当接收到所有上游的同一个版本的 stream barriers ，算子才会进行数据保存
    - 算子接收到一个上游分区数据的检查点时并不会保持数据，而是等到所有的上游分区的同id的 stream barriers 元素到来才进行保存，在这个等待的过程中算子不再继续计算数据，因为会导致多个算子的状态不一致，但是会把上游后续发来的数据进行缓存，等到检查点数据保存完成后继续进行处理
+ 检查点的数据默认保存在 JobManager 的内存，但是也可以修改配置保存到分布式文件系统。尽管可以将数据保存在文件系统，但是一般只有最新的savepoint数据有用，往期的数据可以删除，至于需要保存多少个版本的历史数据，flink可以配置
+ checkpoint本身可以配置一个超时时间，如果执行checkpoint的时间超过了超时时间，还在进行中的 checkpoint 操作就会被抛弃
+ checkpoint还可以配置一个最小间隔时间，如果值设置为了 5000， 无论 checkpoint 持续时间与间隔是多久，在前一个 checkpoint 完成时的至少五秒后会才开始下一个 checkpoint。这种机制保证程序不会非常频繁的checkpoint，浪费性能。
+ 一个Job中同一时间一般只有一个checkpoint在执行，这保证系统不会因为过多的checkpoint影响计算，但是有时候并行的checkpoint也是有意义的，需要配置。注意当checkpoint的并行配置和最小间隔时间配置不能同时存在
## SavePoint
+ Savepoint 是依据 Flink checkpointing 机制所创建的流作业执行状态的一致镜像，savepoint 和 checkpoint原理上很相似，用法也很相似，但是它们处理不同的场景，很多特性也不相同
+ Checkpoint 的生命周期由 Flink 管理，数据的管理和删除无需用户干预，默认情况下checkpoint会在作业完成或者取消后自动删除。savepoint由用户创建，拥有和删除，是用户有意识有计划的进行的数据备份，所以其数据必须在作业停止后继续存在，数据的恢复成本也更高
## State Backend
+ checkpoint和savepoint的数据都是需要持久化才可以实现容错，为了防止数据丢失，保障恢复时的一致性，需要选择一个安全的位置保存数据，flink提供 State Backend 机制，将数据的持久化方式抽象为StateBackend，我们可以的创建各种StateBackend，将状态数据以不同的方式保存在不同的位置
+ Flink提供了三种内置的State Backend用于我们保存状态数据
    - MemoryStateBackend:将正在运行中的状态数据保存在 TaskManager 的内存中， CheckPoint的状态快照发送到 JobManager保存在其堆内存中。优点是速度快;缺点是大小受 JobManager 的内存限制，且数据易丢失
    - FsStateBackend:将正在运行中的状态数据保存在 TaskManager 的内存中，CheckPoint的状态快照写入到配置的文件系统目录中。优点是状态访问和更新基于内存，速度快;缺点是状态大小受集群可用内存限制
    - RocksDBStateBackend:将正在运行中的状态数据保存在 RocksDB 数据库中，RocksDB 数据库默认将数据存储在 TaskManager 的数据目录，CheckPoint 时整个RocksDB 数据库被checkpoint到配置的文件系统目录中。优点是存储空间是可扩展的，状态大小基本没有限制，还可以使用增量快照;缺点是状态的访问和更新必须经过序列化和反序列化，而且都需要经过磁盘读写，速度比较慢，会降低系统整体吞吐量
    - RocksDBStateBackend的增量快照:不同于产生一个包含所有数据的全量备份，增量快照中只包含自上一次快照完成之后被修改的记录，因此可以显著减少快照完成的耗时，同时增量快照恢复不需要解析 Flink 的统一快照格式来重建本地的 RocksDB 数据表，直接基于增量文件加载数据，cpu的消耗很小。但是增量快照是基于 RocksDB 内部的一种机制实现，这种实现虽然可以让数据以增量形式保存，但是增量数据和以往快照之间可能存在数据重合，快照体积可能会变大，如果系统的网络是瓶颈，增量快照恢复可能会消耗更多时间(需要注意的是，一旦启用了增量快照，网页上展示的 Checkpointed Data Size 只代表增量上传的数据量)

# 状态一致性
+ 在分布式系统中，数据传递的一致性语义一般分为三种,最多一次，最少一次和精确一次。Flink可以通过 checkPoint 实现端到端的精确一次，因为出现异常重启以后，flink会从checkpoint读取上一次的状态和数据的偏移量，再次处理，相当数据和计算回滚到了最近一checkpoint的位置
+ 在使用checkpoint机制实现一致性语义的时候，可以选择CheckpointingMode，对于大多数应用来说，精确一次是较好的选择，至少一次可能与某些延迟超低（始终只有几毫秒）的应用的关联较大


# 广播变量
+ Broadcast是一份存储在TaskManager内存中的只读的缓存数据，用于在执行job的过程中需要反复使用的数据，为了达到数据共享，减少运行时内存消耗，我们就用广播变量进行广播




遗留问题
1. 快照清理
2. Broadcast State 模式 的作用
3. savepoint 的使用