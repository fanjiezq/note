# 分布式并行执行
+ flink程序在集群中一般是分布式并行执行的，程序的每个算子会形成一个Task(也可能多个算子合并成为一个Task)，这个Task是逻辑上的概念，在实际运行时，每个Task又会被分为多个subTask，具体分为多少个subTask要看这个算子设置的并行度 setParallelism()
+ flink程序一般使用KeydStream来实现并行，根据key不同将不同的数据分配给下游的算子的子任务，此时Key的数量应该大约等于下游算子设置的并发度，否则下游算子的部分子任务实际上并没有工作。 所以在实际中，程序的实际并行度不单单由并行度参数决定
+ 无论是KeydStream还是一般的Steam，流中数据发送到哪个下游的subtask节点是由flink自行决定。有些情况我们想要控制这一过程让数据按照我们的要求进行分配，flink也提供了这种API，一般称为物理分区。具体方法如下：
    - Steam.global: 上游算子将所有记录发送给下游算子的第一个实例。
    - Steam.broadcast: 上游算子将每一条记录发送给下游算子的所有实例。
    - Steam.forward：只适用于上游算子实例数与下游算子相同时，每个上游算子实例将记录发送给下游算子对应的实例。
    - Steam.shuffle：上游算子对每条记录随机选择一个下游算子进行发送。
    - Steam.rebalance：上游算子通过轮询的方式发送数据。
    - Steam.rescale：当上游和下游算子的实例数为 n 或 m 时，如果 n < m，则每个上游实例向ceil(m/n)或floor(m/n)个下游实例轮询发送数据；如果 n > m，则 floor(n/m) 或 ceil(n/m) 个上游实例向下游实例轮询发送数据。
    - Steam.partitionCustomer：当上述内置分配方式不满足需求时，用户还可以选择自定义分组方式

