# 基本介绍
+ spark是一种基于内存的,准实时的,迭代流式计算框架
+ spark的数据保存在内存中,相比与hadoop速度有着很大提升;spark的计算是迭代式的,即上一步的计算输出作为下一步的输入,层层迭代直到获取到最终结果;
+ spark可以做批处理,也可以进行流处理.但是其本质依旧是批处理思想,为了达到流处理的效果,Spark实现了一种叫做微批（Micro-batch）*的概念,将数据流视作一系列非常小的"批",实现流计算的效果.
+ 因为不是基于真正的流处理思想,spark相比真正的流处理框架(stome,flink)在性能方面依然存在不足,只能提供亚秒级的响应速度

# spark VS hadoop
+ spark 与 hadoop 都基于批处理思想,两者的根本差别是多个作业时间的通信问题:spark是基于内存,hadoop是基于磁盘
+ 在绝大多数场景下,spark的计算速度都比hadoop快,快的代价是大量的磁盘消耗,所以spark适合一些数据量比较小的批处理场景,hadoop适合数据量非常大的批处理场景

# RDD(Resilient Distributed Dataset)弹性分布式数据集
+ 数据集:RDD是一个概念，本质就是数据集合的一种描述，包含的数据的来源，需要计算的函数，对其他RDD的依赖关系
+ 分布式:RDD只是对数据的描述，数据的物理位置是不是在一起并没有什么影响,所以一个RDD的数据可能存在于集群中的不同节点
+ 弹性:RDD的数据默认存储在计算机内存中,但是数据很大的时候内存可能不够用,为了保证容错性,当内存不够用时.spark会将RDD的部分数据写入磁盘,这就是弹性

# RDD五大核心属性
+ 分区列表:每个RDD一般由多个分区组成,不同分区之间的数据互相隔离;分区的主要目的是为了并行计算的管理方便
+ 分区计算函数:RDD除了含有数据还含有数据的计算逻辑,计算逻辑是与分区绑定,每个分区的数据不同，但是计算函数都是相同的，尽管计算逻辑相同，数据模型定义上每个分区依旧有一个计算函数
+ 依赖的RDD列表:RDD一般都是从其他RDD转换而来的,所以每个RDD都存在父RDD.这样层层依赖,就会形成一个RDD的依赖链条.这种依赖依赖链的方式保存数据的一大好处是不用保存实际的数据,只需要保持产成数据的逻辑,这样任何时候需要用到数据只需要根据生成逻辑计算即可
+ 分区器:对RDD数据进行分区的函数,以key-value的形式对数据进行分区
+ 首选位置:分布式计算计算节点一般是随机的，但是也存在特殊需求要求某些RDD的计算任务在特定节点执行或者在某些节点执行效率更高，可以指定任务优先执行的节点


# 核心概念
+ Driver:spark任务驱动程序,用于执行application中的main方法，将用户程序转化为作业(job),在Executor之间调度任务(task),并跟踪Executor执行情况
+ Executor:任务执行器，对应计算机节点一个java进程，Driver会在申请到的资源节点运行Executor进程，然后将task发放给Executor执行，一个Executor可以管理多个Task
+ 算子:一个RDD一般是由其他RDD经过一系列计算逻辑封装转化而来，这些计算逻辑就是算子，算子分为两种
    - 转换算子: 只封装转换逻辑，但是不触发实际的转换动作，比如map,sorted
    - 行动算子: 包含转换逻辑同时真正的触发转换操作，Driver遇到行动算子就会新建一个Job开始进行真正的任务调度计算，比如collect,foreach
+ Application:Spark中最顶层的概念，集群管理的单位；业务层面的体现就是我们需要完成的任务，代码层面的体现是一个程序包，我们可以将一个Application提交到Spark集群，由集群管理运行
+ Job:作业。一个Applicaiton可能要达到多个目标，得到多个结果，那么得到每个结果的过程就是一个Job。一个Application当然可以有多个job，集群在执行程序时，每遇到一个Action算子都会新建一个Job。一个Spark的作业通常比较复杂，包含大量的数据和多种转换聚合的过程，为了简化管理，我们可以将 Job 进一步细化。细化的方向有纵向和横向两种，Spark采用两种结合的方式。
+ Stage:Job的纵向划分，将一个Job工作流程切成更短小的段，切分的依据是RDD的依赖关系。RDD之间存在依赖关系，多个RDD会形成一个依赖链条。联调中RDD数据依赖分为全量依赖（map）和部分依赖（groupByKey），部份依赖一般需要由多个 RDD聚合成一个 RDD，所以多个RDD之间需要进行同步，所以RDD依赖链条中凡是存在部分依赖的地方都被分为两个阶段。
    - RDD一般由其他RDD经过转换算子转换计算得到，所以RDD之间存在依赖关系，依赖关系有两种：
        - 窄依赖(Narrow dependency): 子RDD的partition内容是从父RDD的一个partition中得到.形成一一对应的关系
        - 宽依赖(Shuffle dependency):子RDD的每个partition内容是从父RDD的多个partition中获取,本质就是shuffle
    - RDD最终会被转化为具体的计算任务在Executor执行，任务是根据分区划分的，每个任务处理一个分区数据。如果一个计算任务中所有RDD都是窄依赖，那么每个任务之间并无关联，各自并行执行即可;但是如果任务中存在宽依赖，那么任务到达某个节点必须进行shuffle，多个任务就必须进行同步以完成shuffle。为了管理方便spark引入阶段的概念(Stage)，即每个shuffle前后分为两个阶段，这样一个任务可能会存在多个阶段
    - Stage 有两种 ShuffleMapStage 和 ResultStage
    - Job构建时任务的阶段就划分好了，Spark 在构建Job时会拿到RDD，然后解析RDD的依赖链条，每遇到一个宽依赖，就新建一个阶段，阶段也形成一个链条，从最初的阶段到最终，最后加上一个结果阶段ResultStage形成一个完整的任务阶段。源码位于：DAGScheduler.handleJobSubmitted -> createResultStage
    - 存在依赖关系的Stage的运行是有顺序的,上游的Stege必须全部执行完成产出数据,下游的Stage才可以运行,因为Shuffle需要所有的partition数据都准备好
+ Task:Job的横向划分，将一个Job切成更细的线。因为RDD的数据是分区的，而且通常情况下，一个业务流程中对数据的处理是按照分区数据处理的，在整个job的流程中，分区于分区之间的数据处理流程是互不干扰的，那么我们可以将每个分区的数据处理流程看作一个更小的作业，就是Task。事实上Spark的Task很多时候并没有贯穿一整个Job的流程，而是被划分在Stage下，所以每个Task的更短更小。
    - 与Stage对应，Task也分为两种，ShuffleMapTask 和 ResultTask。
    - 每个Stage中最后一个分区的个数就是Task个数。源码位于:DAGScheduler.submitMissingTasks -> partitionsToCompute
+ 如果将Spark集群看作一个工厂，Application就是一个需求，Job是完成需求的整个生产线，Stage 是生产线上的每个部门，Task是每个部门内部的多条流水线


# RDD之间的依赖关系
RDD一般都是由其他RDD转换而来，RDD之间会形成依赖链(compute chain) ,除了起点RDD其他RDD都存在 parents RDD,一个RDD和其parnets 的依赖关系可以分为两种宽依赖和窄依赖
 + 窄依赖(1:1)：一个RDD只有一个parnet RDD
 + 宽依赖(N:1)：一个RDD的存在多个 parent RDD
实际上，RDD之间的依赖关系可以进一步细化 partition 级别，即一个RDD的一个partition和parent的partition之间的依赖关系其关系也分为两种，完全依赖和部分依赖:
 + 完全依赖(NarrowDependency)：不论窄依赖或者宽依赖，不论子RDD的一个partition来自一个或者多个partition，但是计算子partition需要父partition的全部数据
 + 部分依赖(ShuffleDependency)：不论窄依赖或者宽依赖，不论子RDD的一个partition来自一个或者多个partition，但是计算子partition需要只需要


# shuffle
+ spark的shuffle过程数据的分类是在map阶段进行的,map阶段将数据按照数据key进行归类,并持久化达到磁盘,reducer只需要从磁盘拉去自己需要的数据即可
+ reducer必须等到所有mapper数据准备好了以后才可以拉取,其实也可以做到每个mapper准备好了就直接拉取,但是为了迎合 stage 的概念,还是选择全部 ShuffleMapTasks 执行完再去 fetch
+ reducer是边拉取数据边处理的,拉取数据也是一次一次的,每次拉取的数据会先缓存起来,每次拉取的数据不会太多,否则会超出缓存
+ 每个ShuffleMapStage 形成后，会将该 stage的final RDD 注册到 MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size),reducer就可以从 MapOutputTrackerMaster 询问到 ShuffleMapTask 输出的数据位置

# 任务执行流程
1. spark 任务执行分为两个步骤，资源申请和任务计算,任务提交到spark集群后，框架会向集群申请资源节点，不管是单纯的spark集群还YARN管理的集群，其中一个节点运行Driver，其他节点运行Executor
2. 申请到资源后，Driver进程首选运行，Driver执行main方法,直到遇到action算子,开始根据计算逻辑构建Job,Job的构建很复杂,首先需要根据计算逻辑构建逻辑执行图,即组织和记录RDD之间的依赖关系;然后根据逻辑执行图构建物理执行图,即划分JOb的Stage和Task
3. Job的物理执行图构建完成以后就可以提交到集群了,提交job实际上是提交Task队列到任务调度器.任务调度器将接受到的任务队列保存,然后根据申请到的资源把Task分配到Executor执行
4. 需要注意的是,RDD的分区内数据执行是有序的,分区中的每个元素会走过计算链中的所有计算逻辑,下一个元素才会开始计算

# 序列化 和 闭包检测
+ 从计算的角度看,Spark程序算子之外的代码是在Driver端执行的,算子里面的代码是在Executor执行的,算子中如果用到了外部变量,变量必须进行可以进行序列化,因为需要在网络中传输.
+ 算子引用外部变量会触发闭包操作,即算子内部的变量是外部变量复制体,会随着Task的拆分封装在包内部.因为Executor不可能每次计算都从Driver引用数据然后再进行返回,性能太低.所以闭包引用的变量必须序列化,否则执行会报错
+ 理论上只有Task执行的时候才可以感知到闭包中的变量是否可以序列化.但是实际上spark存在闭包检测功能,在执行之前就会检测闭包中的变量是否可以序列化.因为算子中引用了外部的变量,变量必然会被发送到Executor,那么这些变量必须能够序列化


# RDD数据持久化
+ RDD是数据和计算逻辑的描述，并不会真正的保存数据，所以一般情况下RDD是无法进行传统的数据重用的.但是很多情景是需要数据重用的，spark的解决办法是使用额外手段在内存或者磁盘中计算并保存下某个RDD描述的数据，让后续依赖此RDD的RDD直接使用保存下来的数据达到数据持久化的效果
+ RDD 的数据持久化除了让数据重用，还可用于保存一些计算时间比较长的RDD节点，避免后续RDD出现错误时从头开始计算。
+ RDD 持久化有两种方式
    - RDD.persist():用于将RDD的数据保存内存或者磁盘，具体保存策略可以由参数配置。RDD调用此方法后，后续依赖此RDD和RDD 会直接从缓存中获取数据,此方式使用内存或者临时文件保存数据，任务完成后，数据就被删除
    - RDD.checkPoint():将数据保存在磁盘，此方式需要指定保存位置，一般在分布式存储中。作业执行完成后数据不会被删除，可以跨应用复用数据;而且此方式的保存操作是独立运行的，也就是的会有一个额外的任务被创建专门用于生成需要保存的RDD然后进行保存
    - RDD.persist() 和 RDD.checkPoint() 各有优劣，RDD.persist()数据可以保存在内存，速度快，安全性低;也可以保存在磁盘，速度慢，安全性高，但是任务执行完毕会丢失。 RDD.checkPoint() 直接保存在磁盘，且会产生额外任务，数据不会丢失，但是速度最慢
    - 为了提升RDD.checkPoint()效率， 一般 RDD.persist() 和 RDD.checkPoint() 结合使用，这样RDD.persist()先执行，RDD.checkPoint() 跟随执行，可以减少一次执行成本
    - RDD.persist() 的实现原理是在RDD的依赖关系中添加了一个额外的RDD依赖，代表缓存依赖
    - RDD.checkPoint() 的实现原理是切断了RDD原有的依赖关系，建立新的依赖，将原有的一系列依赖替换为一个checkPointRDD


# 累加器 和 广播变量
+ 因为spark的任务是分布式执行的，普通的变量无法做到集群间的共享，Spark提供累加器 和 广播变量来实现集群数据共享
+ 累加器是分布式共享只写变量，一般用于集群数据统计，“只写”的侧重点是每个 Executor 只能读取自己本地的累加器变量，并不能读取全局的累加器变量,只有Driver能读取到全局累加器变量
+ 广播变量是分布式共享只读变量，用于同一个Executor之间多个Task之间的数据共享，避免同一节点的多个Task保存多份相同的数据
