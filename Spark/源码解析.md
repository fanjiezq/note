# 环境相关(Yarn集群)
## 集群资源申请流程
1. 执行提交命令后,集群会运行 SparkSubmit类的main方法. SparkSubmit完成参数解析(SparkSubmitArguments);判断命令目的(杀死任务或者提交任务); 
2. 判断集群模式,准备提交环境:SparkSubmit 的main方法运行后会判断集群环境,并启动 SparkApplication.如果是Standalone模式就启动 ClientApp , 如果Yarn集群模式就 启动 YarnClusterApplication

        private[spark] class SparkSubmit extends Logging {
            ...
            if (args.isStandaloneCluster) {
                if (args.useRest) {
                    childMainClass = REST_CLUSTER_SUBMIT_CLASS
                    childArgs += (args.primaryResource, args.mainClass)
                } else {
                    // In legacy standalone cluster mode, use Client as a wrapper around the user class
                    childMainClass = STANDALONE_CLUSTER_SUBMIT_CLASS
                    if (args.supervise) { childArgs += "--supervise" }
                    Option(args.driverMemory).foreach { m => childArgs += ("--memory", m) }
                    Option(args.driverCores).foreach { c => childArgs += ("--cores", c) }
                    childArgs += "launch"
                    childArgs += (args.master, args.primaryResource, args.mainClass)
                }
                if (args.childArgs != null) {
                    childArgs ++= args.childArgs
                }
            }

            if (isYarnCluster) {
                childMainClass = YARN_CLUSTER_SUBMIT_CLASS
                if (args.isPython) {
                    childArgs += ("--primary-py-file", args.primaryResource)
                    childArgs += ("--class", "org.apache.spark.deploy.PythonRunner")
                } else if (args.isR) {
                    val mainFile = new Path(args.primaryResource).getName
                    childArgs += ("--primary-r-file", mainFile)
                    childArgs += ("--class", "org.apache.spark.deploy.RRunner")
                } else {
                    if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
                    childArgs += ("--jar", args.primaryResource)
                    }
                    childArgs += ("--class", args.mainClass)
                }
                if (args.childArgs != null) {
                    args.childArgs.foreach { arg => childArgs += ("--arg", arg) }
                }
            }
        }



## spark任务计算流程
1. 资源申请完成以后,集群会继续执行dirver线程,会运行到我们自己编写的spark程序main方法,Driver运行main方法,最终会执行到一个行动算子,然后触发SparkContext.runJob() => DAGScheduler.runJob()
    
        def runJob[T, U: ClassTag](
            rdd: RDD[T],
                func: (TaskContext, Iterator[T]) => U,
                partitions: Seq[Int],
                resultHandler: (Int, U) => Unit): Unit = {
                if (stopped.get()) {
                throw new IllegalStateException("SparkContext has been shutdown")
            }
            val callSite = getCallSite
            val cleanedFunc = clean(func)
            logInfo("Starting job: " + callSite.shortForm)
            if (conf.getBoolean("spark.logLineage", false)) {
                logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
            }
            dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
            progressBar.foreach(_.finishAll())
            rdd.doCheckpoint()
        }

2. DAGScheduler 最终会提交job到自己的事件处理队列,DAGScheduler本身存在一个后台线程,不断的从自己的事件队列中获取job处理

         def submitJob[T, U](
            rdd: RDD[T],
            func: (TaskContext, Iterator[T]) => U,
            partitions: Seq[Int],
            callSite: CallSite,
            resultHandler: (Int, U) => Unit,
            properties: Properties): JobWaiter[U] = {
            // Check to make sure we are not launching a task on a partition that does not exist.
            val maxPartitions = rdd.partitions.length
            partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>
            throw new IllegalArgumentException(
                "Attempting to access a non-existent partition: " + p + ". " +
                "Total number of partitions: " + maxPartitions)
            }

            val jobId = nextJobId.getAndIncrement()
            if (partitions.isEmpty) {
            val clonedProperties = Utils.cloneProperties(properties)
            if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) {
                clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm)
            }
            val time = clock.getTimeMillis()
            listenerBus.post(
                SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties))
            listenerBus.post(
                SparkListenerJobEnd(jobId, time, JobSucceeded))
            // Return immediately if the job is running 0 tasks
            return new JobWaiter[U](this, jobId, 0, resultHandler)
            }

            assert(partitions.nonEmpty)
            val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
            val waiter = new JobWaiter[U](this, jobId, partitions.size, resultHandler)
            eventProcessLoop.post(JobSubmitted(
            jobId, rdd, func2, partitions.toArray, callSite, waiter,
            Utils.cloneProperties(properties)))
            waiter
        }

        /**
        * The main event loop of the DAG scheduler.
        * 循环获取事件处理,最终会调用 handleJobSubmitted() 方法处理job
        */
        override def onReceive(event: DAGSchedulerEvent): Unit = {
            val timerContext = timer.time()
            try {
                doOnReceive(event)
            } finally {
                timerContext.stop()
            }
        }

3. 在 DAGScheduler.handleJobSubmitted() 方法中会进行以下操作:
    1. 得到 ResultStage : getOrCreateShuffleMapStage()
    2. 从 ResultStage 往前查找,找到没有父 Stage 的一个先提交运行 : submitStage(),submitMissingTasks()
    3. 第一个Stage执行完成以后,再提交下一个Stage.每个Sagte和Stage之间都存在shuffle操作,所以必须有顺序
    4. submitMissingTasks() 中会分解Stage,细化为 tasks,最终调用 TaskScheduler.submitTasks() 提交运行
    5. 在获取 ResultStage 的时候,  ResultStage 的 final Rdd注册到MapOutputTrackerMaster,也就是将stage输出结果的位置保存在这里,以便下一个Stage的tasks们能够获取到数据:mapOutputTracker.registerShuffle()


# 组件通信
1. 最新版的spark采用Netty作为通信框架,在SparkContxt中提前准备好通信环境
    + Driver : 启动Netty服务器
    SparkContxt.createSparkEnv() -> createDriverEnv() -> create() -> RpcEnv.create -> NettyRpcEnvFactory().create()
        private[rpc] class NettyRpcEnvFactory extends RpcEnvFactory with Logging {

            def create(config: RpcEnvConfig): RpcEnv = {
                val sparkConf = config.conf
                // Use JavaSerializerInstance in multiple threads is safe. However, if we plan to support
                // KryoSerializer in future, we have to use ThreadLocal to store SerializerInstance
                val javaSerializerInstance =
                new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
                //创建通信环境
                val nettyEnv =
                new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
                    config.securityManager, config.numUsableCores)
                if (!config.clientMode) {
                    val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = { actualPort =>
                        //启动服务器
                        nettyEnv.startServer(config.bindAddress, actualPort)
                        (nettyEnv, nettyEnv.address.port)
                    }
                    try {
                        Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
                    } catch {
                        case NonFatal(e) =>
                        nettyEnv.shutdown()
                        throw e
                    }
                }
                nettyEnv
            }
        }
2. Driver和Executor 通过 RpcEndpoint 和  RpcEndpointRef 分别进行数据的接受(receive)和发送(ask), 这两个组件以收件箱和发件箱的方式进行消息的管理;服务启动以后 RpcEndpoint需要注册到 Dispatcher ,进行统一调度,每注册一个 Server,Dispatcher都会为其分配一个收件箱(Inbox),此收件箱由 Dispatcher 统一管理. 

        private[netty] class Dispatcher(nettyEnv: NettyRpcEnv, numUsableCores: Int) extends Logging {
            //收件箱管理者
            private val endpoints: ConcurrentMap[String, MessageLoop] = new ConcurrentHashMap[String, MessageLoop]
            private val endpointRefs: ConcurrentMap[RpcEndpoint, RpcEndpointRef] = new ConcurrentHashMap[RpcEndpoint, RpcEndpointRef]


            def registerRpcEndpoint(name: String, endpoint: RpcEndpoint): NettyRpcEndpointRef = {
                ...
                var messageLoop: MessageLoop = null
                try {
                    messageLoop = endpoint match {
                    case e: IsolatedRpcEndpoint =>
                        new DedicatedMessageLoop(name, e, this)
                    case _ =>
                        sharedLoop.register(name, endpoint)
                        sharedLoop
                    }
                    // 创建收件箱DedicatedMessageLoop 并且放入收件箱管理器
                    endpoints.put(name, messageLoop)
                } catch {
                    case NonFatal(e) =>
                    endpointRefs.remove(endpoint)
                    throw e
                }
                endpointRef
            }
        }
4. 相应的,每个 RpcEndpointRef 也有一个发件箱(Outbox) ,被 NettyRpcEnv 管理着,与收件箱不同的是,收件箱只有一个,发件箱却是根据 RpcAddress 区分存在多个,因为需要给多个节点发送消息
        /**
        * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],
        * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.
        */
        private val outboxes = new ConcurrentHashMap[RpcAddress, Outbox]()
5. 收件箱和发件箱内部最终都是通过 TransportClient 工具类进行消息的收发

# 应用程序执行
## RDD依赖
MapPartitonRDD  <--------OneToOneDependency---- MapPartitonRDD <-------ShuffleDependency------ ShuffleRDD

## 阶段划分
Driver运行Spark的行动算子触发Job提交,调度器构建Job对象,会进行阶段划分 : SaprkContext.runJob() -> DAGScheduler.runJob() -> DAGScheduler.handleJobSubmitted()
1. 将当前运行行动算子的RDD传递到调度器: SaprkContext.runJob() 会将运行行动算子的RDD传递到调度器.最终到达 DAGScheduler.createResultStage() 
2. 获取结果阶段ResultStage:DAGScheduler.createResultStage()
    1. 根据当前RDD获取或者创建上级阶段:从当前RDD往前寻找依赖的RDD,遇到ShuffleDependency 就创建一个ShuffleMapStage;遇到ShuffleDependency 同时又会将当前RDD作为最终的RDD继续走同样的流程继续往前找,一直找到数据源
            
            private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
                getShuffleDependencies(rdd).map { shuffleDep =>
                    getOrCreateShuffleMapStage(shuffleDep, firstJobId)
                }.toList
            }

            def createShuffleMapStage[K, V, C](
                shuffleDep: ShuffleDependency[K, V, C], jobId: Int): ShuffleMapStage = {
                val rdd = shuffleDep.rdd
                checkBarrierStageWithDynamicAllocation(rdd)
                checkBarrierStageWithNumSlots(rdd)
                checkBarrierStageWithRDDChainPattern(rdd, rdd.getNumPartitions)
                val numTasks = rdd.partitions.length
                // 获取到 ShuffleMapStage 后继续往前找
                val parents = getOrCreateParentStages(rdd, jobId)
                val id = nextStageId.getAndIncrement()
                val stage = new ShuffleMapStage(
                id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker)

                stageIdToStage(id) = stage
                shuffleIdToMapStage(shuffleDep.shuffleId) = stage
                updateJobIdStageIdMaps(jobId, stage)

                if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
                // Kind of ugly: need to register RDDs with the cache and map output tracker here
                // since we can't do it in the RDD constructor because # of partitions is unknown
                logInfo(s"Registering RDD ${rdd.id} (${rdd.getCreationSite}) as input to " +
                    s"shuffle ${shuffleDep.shuffleId}")
                mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)
                }
                stage
            }

    2. 创建结果RDD

            private def createResultStage(rdd: RDD[_],func: (TaskContext, Iterator[_]) => _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage = {
                checkBarrierStageWithDynamicAllocation(rdd)
                checkBarrierStageWithNumSlots(rdd)
                checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)
                //获取上级阶段
                val parents = getOrCreateParentStages(rdd, jobId)
                val id = nextStageId.getAndIncrement()
                //创建结果阶段
                val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
                stageIdToStage(id) = stage
                updateJobIdStageIdMaps(jobId, stage)
                stage
            }

3. 提交ResultStage:ResultStage其实已经包含了整个Job所有的Stage.提交ResultStage后,调度器会从后往前找,找到没有的父级的Stage真正的进行提交.
    
        private def submitStage(stage: Stage): Unit = {
            val jobId = activeJobForStage(stage)
            if (jobId.isDefined) {
            logDebug(s"submitStage($stage (name=${stage.name};" +
                s"jobs=${stage.jobIds.toSeq.sorted.mkString(",")}))")
            if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
                val missing = getMissingParentStages(stage).sortBy(_.id)
                logDebug("missing: " + missing)
                if (missing.isEmpty) {
                    logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
                    // 提交没有父级的Stage
                    submitMissingTasks(stage, jobId.get)
                } else {
                    for (parent <- missing) {
                        submitStage(parent)
                    }
                    waitingStages += stage
                }
            }
            } else {
            abortStage(stage, "No active job for stage " + stage.id, None)
            }
        }

## 任务切分
1. Job阶段划分完成进行提交,但是最终提交到Executor的并不是Job而是Task,所以在提交Job的最后一步还有一个任务的划分阶段(DAGScheduler.submitMissingTasks()),任务切分阶段会从ResultStage往前找,一直找到没有父级阶段的Stage首先进行划分,一般都是 ShuffleMapStage.每个阶段的Task数量都是本阶段最后一个RDD的分区数量,Job总的Task数量是所有阶段Task数量之和:

        private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
        ...
            val tasks: Seq[Task[_]] = try {
            val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
            stage match {
                case stage: ShuffleMapStage =>
                stage.pendingPartitions.clear()
                partitionsToCompute.map { id =>
                    val locs = taskIdToLocations(id)
                    val part = partitions(id)
                    stage.pendingPartitions += id
                    new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
                    taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
                    Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
                }

                case stage: ResultStage =>
                partitionsToCompute.map { id =>
                    val p: Int = stage.partitions(id)
                    val part = partitions(p)
                    val locs = taskIdToLocations(id)
                    new ResultTask(stage.id, stage.latestInfo.attemptNumber,
                    taskBinary, part, locs, id, properties, serializedTaskMetrics,
                    Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
                    stage.rdd.isBarrier())
                }
            }
            } catch {
            case NonFatal(e) =>
                abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
                runningStages -= stage
                return
            } 

        }

## 任务调度
任务切分完成以后,会形成一个任务列表发往调度中心,继而调度到各个Executor执行运算.需要如下几个步骤:
1. 提交任务列表到任务调度器: DAGScheduler.submitMissingTasks() -> taskScheduler.submitTasks()
2. 任务调度器将任务封装为TaskSetManager投放到任务池:TaskSchedulerImpl.submitTasks(),然后从任务池取出任务准备发布到Executor

        override def submitTasks(taskSet: TaskSet): Unit = {
            val tasks = taskSet.tasks
            logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
            this.synchronized {
            val manager = createTaskSetManager(taskSet, maxTaskFailures)
            val stage = taskSet.stageId
            val stageTaskSets =
                taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])

            stageTaskSets.foreach { case (_, ts) =>
                ts.isZombie = true
            }
            stageTaskSets(taskSet.stageAttemptId) = manager
            //封装任务并投放到任务池
            schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)

            if (!isLocal && !hasReceivedTask) {
                    starvationTimer.scheduleAtFixedRate(new TimerTask() {
                    override def run(): Unit = {
                        if (!hasLaunchedTask) {
                        logWarning("Initial job has not accepted any resources; " +
                            "check your cluster UI to ensure that workers are registered " +
                            "and have sufficient resources")
                        } else {
                        this.cancel()
                        }
                    }
                    }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
                }
                hasReceivedTask = true
            }
            //取出任务准备进行调度
            backend.reviveOffers()
        }
3. 调度并启动任务. SchedulerBackend.reviveOffers() -> CoarseGrainedSchedulerBackend.receive() -> CoarseGrainedSchedulerBackend.makeOffers() -> CoarseGrainedSchedulerBackend.launchTasks()
4. 任务调度策略:集群任务调度是很复杂的,要考虑资源,效率,计算顺序等等.针对Task的执行顺序,Spark存在两种调度算法(SchedulingMode.FIFO 和 SchedulingMode.FAIR).针对效率,spark根据数据的位置选择选择执行的节点,尽可能选择离数据近的节点
5. 最终选定了每个任务的执行节点,将任务序列化发送到对应的节点进行执行:CoarseGrainedSchedulerBackend.launchTasks()

## 任务执行
集群的Work节点长期运行一个ExecutorBackend时刻准备接收Executor发送的任务.任务的执行的很简单,Executor存在一个线程池,任务到来以后直接拿出一个线程计算即可
        
    private[spark] class CoarseGrainedExecutorBackend() extends IsolatedRpcEndpoint with ExecutorBackend with Logging {
        ...
        override def receive: PartialFunction[Any, Unit] = {
            ...
            case LaunchTask(data) =>
                if (executor == null) {
                    exitExecutor(1, "Received LaunchTask command but executor was null")
                } else {
                    val taskDesc = TaskDescription.decode(data.value)
                    logInfo("Got assigned task " + taskDesc.taskId)
                    taskResources(taskDesc.taskId) = taskDesc.resources
                    executor.launchTask(this, taskDesc)
                }
        }
    
    }

    private[spark] class Executor(){
        ...
        def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
            val tr = new TaskRunner(context, taskDescription)
            runningTasks.put(taskDescription.taskId, tr)
            threadPool.execute(tr)
        }
    }

# Shuffle
## 原理和执行过程
Spark的job执行是分阶段(Stage)的,Stage是根据是否存在Shuffle操作划分的,每遇到一个Shuffle操作就新增一个Stage.Stage的执行是有顺序的,只有上一个Stage的所有Task全部计算完成,下一个Stage才可以执行.因为下一个Stage需要上一个Stage 的全部分区数据.顺序依赖意味着每个Stage的计算数据必须完全保存下来给下一个Stage使用.生产环境中数据量一般比较大,每个Stage产生的数据完全保存在内存不现实,Stage计算的结果必须落盘,如此产成的大量的磁盘IO就导致了 Shuffle 性能很低.
## 写磁盘
## 读取磁盘


# RDD相关
## 分区的原因
+ RDD的数据基本都是分区的,分区只是一个概念,为的是将大量的据在逻辑上分开,数据分区的最核心的原因就是方便管理和计算
+ RDD是一个整体概念,一个数据集可能因为某种特性被划分在一起成为一个RDD,这个数据集的数据量可能是很大的,计算时如果作为一个整体的处理就会失去分布式计算的意义,而Spark本就是基于分而治之思想的计算框架,所以恩数据必然需要分区,方便集群将任务拆分


## 分区原则和方法



# 内存的管理
## 内存的分类
## 内存的基本配置
