=========================== hdfs 高可用 ==========================
部署规划
机器名称    namenode    zookeeper    datanode  journalnode
node1      true                               
node2      true        true         true      true
node3                  true         true      true
node4                  true         tuee      true

服务器ssh免密登录
1）root用户安装好ssh
2）每台服务器进入 /root/.ssh/目录(如果没有则创建，赋予700权限) 生成公钥和私钥
    node1 : ssh-keygen -t rsa
    node2 : ssh-keygen -t rsa
    node3 : ssh-keygen -t rsa
    node4 : ssh-keygen -t rsa
3）将公钥信息存放在的 authorized_keys文件(文件权限需要是600)
    node1 : cat id_rsa.pub > authorized_keys
    node2 : ssh-copy-id -i id_rsa.pub root@node1
    node3 : ssh-copy-id -i id_rsa.pub root@node1
    node4 : ssh-copy-id -i id_rsa.pub root@node1
4）将node1的/root/.ssh/authorized_keys文件拷贝至其他所有服务器的相同目录下
    node1 : scp authorized_keys root@node2:/root/.ssh/
    node1 : scp authorized_keys root@node3:/root/.ssh/
    node1 : scp authorized_keys root@node4:/root/.ssh/
5）在node2 使用 ssh root@node1 测试登录，如果不需要输入密码就可以登录上，表明免密登录设置成功

zookeeper 集群搭建
1）将zookeeper安装包上传至 /usr/local/include下，并解压，进入到解压后的目录（假设为zookeeper-3.4.13）
2）创建自定义配置文件 进入 conf 文件夹，拷贝配置文件模板。cp zoo_sample.cfg zoo.cfg
3) 修改配置项 dataDir=/usr/local/include/zookeeper-3.4.13/data 
4) 在配置文件结尾加上如下配置
    server.1=node2:2888:3888
    server.2=node3:2888:3888
    server.3=node4:2888:3888
5) 新建data 目录:mkdir -p /usr/local/include/zookeeper-3.4.13/data
6) 创建zookeeper标识：进入data目录，运行命令 echo 1 > myid
7) 旧爱那个当前zookpper整体拷贝到node3,node4 相同目录 : scp  -R zookeeper-3.4.13 root@node3:/usr/local/include/
8) 分别修改node3 ，node4 的zookeeper 的唯一标识。cd /usr/local/include/zookeeper-3.4.13/data ; echo  2 > myid
9) 分别启动node2,node3,node4 中的zookeeper
10）在node2使用zkCli.sh 登录至zk，创建节点，在node2 或者 node3 下同样登录，查看节点，如果能够查询到node1上面创建的节点，说明zk集群搭建成功

hadoop集群搭建
1）上传hadoop包到node1的/usr/local/include 目录下并解压（假设为 hadoop-2.7.2），然后进入配置文件目录 cd /usr/local/include/hadoop-2.7.2/etc/hadoop
2）配置core-site.xml
<configuration>
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://mycluster</value>
    </property>

    <property>
      <name>hadoop.tmp.dir</name>
      <value>/usr/local/include/hadoop-2.7.2/data/tmp</value>
    </property>

    <property>
		<name>ha.zookeeper.quorum</name>
		<value>node2:2181,node3:2181,node4:2181</value>
    </property>
</configuration>
3）配置hdfs-size.xml
<configuration>

  <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->
  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
    <description>集群服务名，需要和core-site.xml中的配置一致</description>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
    <description>mycluster下面有两个NameNode，分别是nn1，nn2</description>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>172.18.0.101:8020</value>
    <description>nn1的RPC通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>172.18.0.102:8020</value>
    <description>nn2的RPC通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>172.18.0.101:50070</value>
    <description>nn1的HTTP通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>172.18.0.102:50070</value>
    <description>nn2的HTTP通信地址</description>
  </property>

  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://172.18.0.102:8485;172.18.0.103:8485;172.18.0.104:8485/mycluster</value>
    <description>指定NameNode的元数据在JournalNode上的存放位置</description>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/usr/local/include/hadoop-2.7.2/ha/dfs/journalnode</value>
    <description>指定JournalNode在本地磁盘存放数据的位置</description>
  </property>

  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    <description>配置失败自动切换实现方式</description>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
    <description>配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行</description>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
    <description>使用sshfence隔离机制时需要ssh免登陆</description>
  </property>

  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
    <description>开启 namenode 自动切换配置</description>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/usr/local/include/hadoop-2.7.2/ha/dfs/namenode</value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/usr/local/include/hadoop-2.7.2/ha/dfs/datanode</value>
  </property>


  <property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>30000</value>
  </property>
  <property>
    <name>dfs.journalnode.http-address</name>
    <value>0.0.0.0:8480</value>
  </property>
  <property>
    <name>dfs.journalnode.rpc-address</name>
    <value>0.0.0.0:8485</value>
  </property>
</configuration>
4) 配置slaves 节点，将datanode所在节点的ip配置在这个文件，一行一个，不允许有任何其他字符，包括空格
5) 将node1 节点的hadoop安装包分发其他节点
    scp  -R  /usr/local/include/hadoop-2.7.2/ root@node2:/usr/local/include/
    scp  -R  /usr/local/include/hadoop-2.7.2/ root@node3:/usr/local/include/
    scp  -R  /usr/local/include/hadoop-2.7.2/ root@node4:/usr/local/include/
6) 先启动journalnode
    node2 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh start journalnode;
    node3 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh start journalnode;
    node4 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh start journalnode;
7) 格式化namenode（两台namenode 只能格式化其中一台，另一台同步第一台的数据。格式化之前需要先删除集群的data和logs
    node1 ： cd /usr/local/include/hadoop-2.7.2/bin;   ./hdfs namenode -format ; 
             cd /usr/local/include/hadoop-2.7.2/sbin;  ./hadoop-daemon.sh start namenode 
    node2 ： hdfs namenode -bootstrapStandby
8）在一个节点格式化zk集群
    node2 ： hdfs zkfc -formatZK
9）关闭 journalnode
    node2 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh stop journalnode;
    node3 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh stop journalnode;
    node4 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh stop journalnode;
10) 关闭namenode
    node1 ： cd /usr/local/include/hadoop-2.7.2/sbin ; ./hadoop-daemon.sh stop namenode;
11）正式启动集群 
    node1 : cd /usr/local/include/hadoop-2.7.2/sbin ; ./start-dfs.sh 